#!/usr/bin/env python

from xpipeline.core.xtimeseries import XTimeSeries
from xpipeline.core.xdetector import Detector
from xpipeline.core.xdetector import compute_antenna_patterns
from xpipeline.likelihood.xlikelihood import XLikelihoodMap
from xpipeline.core.xtimefrequencymap import XTimeFrequencyMapDict
from xpipeline.core.xtimefrequencymap import csc_XSparseTimeFrequencyMap
from xpipeline.waveform import xinjectsignal
from xpipeline.setuputils import log
from xpipeline.utils import utils
from xpipeline.postprocess import postprocess, prep_data

from gwpy.table import EventTable
from gwpy.timeseries import TimeSeries
from gwpy.signal import filter_design
from functools import reduce

import pandas
import numpy
import argparse
import tables
import os
import string
import random

_injection_analysis_types = ['onsource_injection', 'zero_noise_injection', 'offsource_injection']
_other_event_types = ['background', 'onsource']
_job_types = []
_job_types.extend(_injection_analysis_types)
_job_types.extend(_other_event_types)

def id_generator(x,size=10, chars=string.ascii_uppercase + string.digits +string.ascii_lowercase):
    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))

def parse_commandline():
    parser = argparse.ArgumentParser(description="This executable processes "
                                                 "injection, off source "
                                                 "and on source trials "
                                                 "X-Pipeline triggered search jobs.")
    # Define groups of flags to make the help output ore useful
    required_args = parser.add_argument_group('required named arguments')
    optional_args = parser.add_argument_group('optional arguments')
    required_args.add_argument("-j", "--job-type", help=
                               """
                               Job Type options are {0}
                               """.format(_job_types),
                               required=True)
    required_args.add_argument("-p", "--parameter-file", help=
                               """
                               This file is usually generated by
                               xpipeline-workflow and is in the input
                               directory. Example:
                               input/parameters_off_source_0.txt
                               """,
                               required=True)
    required_args.add_argument("-e", "--event-numbers", help=
                               """
                               This indicates what background timeslide
                               and timestamp to use
                               The file that the event-number picks from
                               is usually listed in the params file and
                               in a two detector analysis has the form
                               1126259462.0 0 0
                               (timestamp, seconds to slide detector 1,
                               slide detector 2
                               """,
                               required=True, type=int, nargs='+')
    required_args.add_argument("-i", "--injection-numbers", help=
                               """
                               This indicates what injections from
                               the injection files you want to process
                               the injection file usually is in the input folder
                               and has the naming convention injection_"waveform_name".txt
                               Each row of the file has the form
                               gps_s, gps_ns, phi, theta, psi, gwb_type, gwb_params
                               1135227846 42243481 2.034325e+00 1.014036e+00 6.620518e-01 chirplet 1.0e-22~0.01~100~0~0~1
                               """,
                               type=int, nargs='+')

    required_args.add_argument("--cluster", action="store_true", default=False,
                                help="After storing the raw sparse time frequency maps"
                                     "read in and create the clusters in thsoe maps")

    required_args.add_argument("--development", action="store_true", default=False,
                                help="Store the raw unclustered sparse time frequency maps"
                                     "of every trial")

    args = parser.parse_args()

    if args.job_type not in _job_types:
        raise parser.error('You can only perform one of the following analyses '
                           '{0}'.format(_job_types))

    if args.job_type == 'injection' and (args.injection_numbers is None):
        raise parser.error('You must specify what injections you want '
                           'to perform if you are doing an injection analysis.')

    return args

args = parse_commandline()

logger = log.Logger('XPIPELINE: Analysis {0}'.format(args.job_type))

# Read parameters
parameters = dict(line.rstrip('\n').split(':', 1) for line in open(args.parameter_file))

applycalibcorrection = parameters['applycalibcorrection']

# How mcuh data are you analyzing (standard is 256s)
block_time = int(parameters['blocktime'])

# Determine channels and frames being used for the analysis
channel_names_file = parameters['channelFileName']
channel_names = [chan.rstrip('\n').split(' ')[0]
                 for chan in open(channel_names_file)]
frame_types = [chan.rstrip('\n').split(' ')[1]
               for chan in open(channel_names_file)]
frame_types = dict(zip(channel_names, frame_types))

ifos = [chan.split(':')[0] for chan in channel_names]

# Minimum and Maximum search frequency range
minimum_frequency = int(parameters['minimumfrequency'])
maximum_frequency = int(parameters['maximumfrequency'])

# output directory
if args.development:
    # in run directory
    output_directory = os.path.join('.','output')
else:
    # fast i/o location
    output_directory = os.path.join(os.getenv('TMPDIR', None), id_generator(1), 'output')

if not os.path.isdir(output_directory):
    os.makedirs(output_directory)


# Sample frequency
sample_frequency = float(parameters['samplefrequency'])

# What overlap are you using for our ffts (standard is 50%)
offset_fraction = float(parameters['offsetfraction'])

frame_file = parameters['frameCacheFile']
whitening_length = int(parameters['whiteningtime'])
transient_time = 4.0

if 'circtimeslidestep' in parameters.keys():
    slides = {}
    circ_time_slides = int(parameters['circtimeslidestep'])
    nchannels = len(channel_names)
    if nchannels == 1:
        slides[channel_names[0]] = numpy.zeros(1).astype(int)
    elif nchannels == 2:
        internal_slides = numpy.arange(0, (block_time - 2*transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
    elif nchannels == 3:
        internal_slides = numpy.arange(0, (block_time / 2 - transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
        slides[channel_names[2]] = -internal_slides
    else:
        raise ValueError("The circular time slides are not "
                         "implemented for the requested "
                         "number of detectors: {0}".format(nchannels))
else:
    circ_time_slides = None
    internal_slides = numpy.zeros(1).astype(int)
    slides = {k: internal_slides for k in channel_names}

event_file = parameters['eventFileName']

on_source_end_offset = int(parameters['onsourceendoffset'])
on_source_begin_offset = int(parameters['onsourcebeginoffset'])

sky_positions = parameters['skyPositionList']
sky_positions = EventTable.read(sky_positions, format='ascii',
                                names=['theta', 'phi', 'probability', 'area'])

seed = int(parameters['seed'])
fft_lengths = [float(fft) for fft in parameters['analysistimes'].split(',')]
normalization_factor = numpy.sqrt(fft_lengths[0]/numpy.asarray(fft_lengths))
min_fftlength = min(fft_lengths)
likelihoods = parameters['likelihoodtype'].split(',')

numpy.random.seed(seed)
max_pixels = int(1./offset_fraction * 1./min_fftlength * block_time * (sample_frequency/2 * min_fftlength + 1) *len(ifos)* 0.01)

table_description = {
'dx' : tables.Float64Col(),
'dy' : tables.Float64Col(),
'x0' : tables.Float64Col(),
'y0' : tables.Float64Col(),
'shape' : tables.Int64Col(2),
'phi' : tables.Float64Col(),
'theta' : tables.Float64Col(),
'map_type' : tables.StringCol(20),
'ifo' : tables.StringCol(max([len(i) for i in channel_names])),
'x' : tables.Float64Col(shape=max_pixels),
'y' : tables.Float64Col(shape=max_pixels),
'energy' : tables.ComplexCol(itemsize=16, shape=max_pixels),
}

# presets
events = pandas.read_csv(event_file,header=None, delimiter=' ').values
events = events.astype(int)
event = events[args.event_numbers]
for event_time, time_slide in zip(event[:,0], event[:,1:]):

    # start time of the time-frequency map
    start_time = event_time - block_time / 2;

    # likelihoods to analze
    time_slides = dict(zip(channel_names, time_slide))
    analysis_data = XTimeSeries.retrieve_data(event_time, block_time,
                                              channel_names, sample_frequency*2,
                                              frame_types=frame_types, time_slides=time_slides)

    # Are we doing injections and if so how many injection scales are we doing?
    if args.job_type in _injection_analysis_types:
        logger.info('Injecting a signal... Reading injection specific parameters')
        injection_file_name = parameters['injectionFileName']
        waveform_name = injection_file_name.split('_')[1].split('.')[0]
        injection_scales = [float(i) for i in parameters['injectionScales'].split(',')]
        injection_numbers = args.injection_numbers
        catalogdirectory = parameters['catalogdirectory']
    else:
        # If not doing injections there are no injections scales (similarly there
        # are no injection numbers to be done)
        injection_scales = [None]
        injection_numbers = [None]

    # Loop over injections
    for injection_number in injection_numbers:
        if args.job_type in _injection_analysis_types:
            logger.info('Creating injection number {0}'.format(injection_number))
            [injection_data, gps_s, gps_ns, phi_inj, theta_inj, psi_inj] = xinjectsignal.xinjectsignal_fromfile(start_time=start_time, block_time=block_time, channels=channel_names, injection_file_name=injection_file_name, injection_number=injection_number, sample_rate=sample_frequency*2, catalogdirectory=catalogdirectory)

        # Loop over injection scales
        for injection_scale in injection_scales:
            if args.job_type in _injection_analysis_types:
                logger.info('Scaling amplitude by injection scale: {0}'.format(injection_scale))
                scaled_injection_data = XTimeSeries()
                for k, v in injection_data.items():
                    scaled_injection_data[k] = v * injection_scale

                if args.job_type == 'zero_noise_injection':
                    logger.info('Injecting into zero noise data')
                    zero_noise_data = XTimeSeries()
                    for channel in channel_names:
                        zero_noise_data.append({channel : TimeSeries(numpy.zeros(int(sample_frequency * block_time)),
                                                                   channel=channel,
                                                                   name=channel,
                                                                   dt=1./sample_frequency,
                                                                   t0 = start_time)})
                    zero_noise_injection = zero_noise_data.inject(scaled_injection_data)
                    data = analysis_data.copy()
                else:
                    logger.info('Injecting into data')
                    data = analysis_data.inject(scaled_injection_data)
            else:
                data = analysis_data

            data = data.resample(sample_frequency)

            # high pass filter the data
            #data = data.highpass(minimum_frequency/2,)
            # Get the ASDs that will be used to whiten the data
            asds = data.asd(whitening_length)

            if args.job_type == 'zero_noise_injection':
                # use real asd to whiten data with no noise
                whitened_timeseries = zero_noise_injection.whiten(asds)
            else:
                whitened_timeseries = data.whiten(asds)

            # Obtain detector specific information
            detectors = {}
            for det in ifos:
                detectors[det] = Detector(det)

            # Pre calculate time delays number of instruments-1 by number of sky positons
            time_delays = numpy.zeros((sky_positions['phi'].size, len(detectors)-1))
            residual_time_delays = numpy.zeros((sky_positions['phi'].size, len(detectors)-1))
            phi = numpy.array(sky_positions['phi'])
            theta = numpy.array(sky_positions['theta'])

            for idx, idetector in enumerate(ifos[1::]):
                time_delays[:, idx] = numpy.round((detectors[idetector].time_delay_from_earth_center_phi_theta(phi, theta) - detectors[ifos[0]].time_delay_from_earth_center_phi_theta(phi, theta))*sample_frequency)
                residual_time_delays[:, idx] = (time_delays[:, idx] - (detectors[idetector].time_delay_from_earth_center_phi_theta(phi, theta) - detectors[ifos[0]].time_delay_from_earth_center_phi_theta(phi, theta))*sample_frequency)/sample_frequency

            # Initialize empty dictionary of sparse time-frequency maps
            all_sparse_maps = {}
            for circular_time_slide in internal_slides:
                all_sparse_maps[circular_time_slide] = {}
                for fft_length in fft_lengths:
                    all_sparse_maps[circular_time_slide][fft_length] = {}
                    for (sky_position, time_delay) in zip(sky_positions, time_delays):
                        phi = sky_position['phi']
                        theta = sky_position['theta']
                        all_sparse_maps[circular_time_slide][fft_length][(phi, theta)] = {}

            whitened_timeseries[channel_names[1]] = numpy.roll(whitened_timeseries[channel_names[1]], -3)

            # Now let's loop
            for idx_fft_length, fft_length in enumerate(fft_lengths):
                ####################
                # Create FFT grams #
                ####################

                # ---- Rescale conditioned data to normalize it appropriately for this fftlength
                if idx>0:
                    whitened_timeseries.update((x, y*(normalization_factor[idx] / normalization_factor[idx-1])) for x, y in whitened_timeseries.items())
                for idx_sky_postion, (sky_position, time_delay, residual_time_delay) in enumerate(zip(sky_positions, time_delays, residual_time_delays)):

                    # shift data and make fftgram
                    # Make a spectrogram that contains phase information
                    fft_grams = whitened_timeseries.fftgram(fft_length).crop_frequencies(minimum_frequency, maximum_frequency + 1/fft_length)

                    # determine how many indices every pixel needs to move
                    # with respect to the internal time slides
                    final_time_bin_idx = fft_grams[channel_names[0]].shape[0]

                    phi = sky_position['phi']
                    theta = sky_position['theta']
                    antenna_patterns = compute_antenna_patterns(['H1', 'L1'], phi, theta,
                                                                antenna_patterns=['f_plus', 'f_cross'])
                    projected_asds = asds.project_onto_antenna_patterns(antenna_patterns,
                                                                        to_dominant_polarization_frame=True)

                    projected_asds_magnitude = projected_asds.calculate_magnitude()
                    # only keep relevant frequencies. i.e. between min and max frequency
                    # and also frequencies relevant for this FFT length.
                    projected_asds_magnitude = projected_asds_magnitude.slice_frequencies(numpy.arange(minimum_frequency,maximum_frequency + 1, 1./fft_length).astype(int))

                    # convert projected asds to unit vectors
                    projected_asds = projected_asds.to_unit()

                    # NOTE we calculate everything from here to the loop over
                    # internal time slides for the non-slide maps
                    # this way when we find pizels for the slide maps
                    # if we undo the slide to the non-slide maps values
                    # then we do not need to recalculate say likelihood maps again
                    # and again

                    ####################################################
                    # Create TF maps and Likelihood Maps               #
                    ####################################################

                    # Make a spectrogram that contains phase information
                    fft_grams[channel_names[1]] = fft_grams[channel_names[1]].phaseshift(residual_time_delay)
                    # Create Dominant Polariztion Frame projected TF map
                    projected_fftmaps = fft_grams.to_dominant_polarization_frame(projected_asds)

                    ####################################################
                    # Find loud pixels and down select maps            #
                    ####################################################
                    # Create spectrogram that contains info of the energy in each pixel
                    energy_maps = fft_grams.power2()

                    # Turn off the bottom 99 percent of pixels (i.e. set to 0)
                    energy_maps_zeroed = energy_maps.blackout_pixels(99)

                    # add incoherent maps together to find the 99 percentile coherent pixels
                    if idx_sky_postion == 0:
                        total_energy_threshold = energy_maps.to_coherent().blackout_pixels(99).energy.min()

                    # Calculate all time slides
                    tindex_findex_slided = {k : zip(numpy.mod(numpy.repeat(numpy.atleast_2d(energy_maps_zeroed[k].nonzero()[0]), len(v),0) + numpy.atleast_2d(v*(1./fft_length)*(1./offset_fraction)).T, final_time_bin_idx).astype(int), numpy.repeat(numpy.atleast_2d(energy_maps_zeroed[k].nonzero()[1]), len(v), 0))  for k,v in slides.items()}

                    # Make every TF pixels pair in all maps
                    all_maps_pixel_pairs = [zip(i[0], i[1]) for k,v in tindex_findex_slided.items() for i in v]
                    array_of_tf_pixels_per_slide_per_det = numpy.asarray(all_maps_pixel_pairs).reshape(-1, internal_slides.size,)

                    # all coherent tf pixels accross all maps
                    all_pixels = numpy.apply_along_axis(lambda x: reduce(lambda x, y: set(x).union(y), x.tolist()), 0, array_of_tf_pixels_per_slide_per_det)

                    for coherent_pixels, circular_time_slide in zip(all_pixels, internal_slides):

                        # make sure that the coherent time frequency pixels are sorted
                        coherent_pixels = sorted(coherent_pixels)

                        # extract the tindices and the findices
                        tindex, findex = zip(*coherent_pixels)
                        tindex = numpy.asarray(tindex)
                        findex = numpy.asarray(findex)

                        # extract the tindices and the findices
                        tindex = {k : tindex for k in channel_names}
                        findex = {k : findex for k in channel_names}

                        # undo internal_time_slide
                        tindex[channel_names[1]] = numpy.mod(tindex[channel_names[1]]-circular_time_slide*fft_length, final_time_bin_idx).astype(int)

                        # Now that we have the pixels that are in each map
                        # let's reconstruct the sparse tf map
                        # Now with pixels from both detectors
                        tmp_map_to_find_total_energy_threshold = fft_grams.to_sparse(tindex, findex, phi=phi, theta=theta).power2().to_coherent()
                        tfpixel_to_keep = numpy.where(tmp_map_to_find_total_energy_threshold.energy>total_energy_threshold)[0]
                        tindex = {k : v[tfpixel_to_keep] for k,v in tindex.items()}
                        findex = {k : v[tfpixel_to_keep] for k,v in findex.items()}
                        surviving_pixels = {k : v.to_sparse(tindex, findex, phi=phi, theta=theta, map_type=k) for k, v in projected_fftmaps.items()}
                        for k,v in surviving_pixels.items():
                            v.projected_asd_magnitude_squared = numpy.square(projected_asds_magnitude[k])

                        surviving_pixels['excess_energy'] = fft_grams.to_sparse(tindex, findex, phi=phi, theta=theta)
                        all_sparse_maps[circular_time_slide][fft_length][(phi, theta)] = surviving_pixels

            # cluster sparse time-frequency maps and save info
            all_triggers = []
            for circular_time_slide in internal_slides:
                all_clusters = postprocess.extract_clusters_from_dict(all_sparse_maps[circular_time_slide],)
                all_clusters['internal_time_slide'] = circular_time_slide
                all_triggers.append(all_clusters.values)

            # Now stack all the background triggers from the various trials and
            # save them as one big DataFrame
            all_triggers = numpy.vstack(all_triggers)
            non_string_cols = all_clusters.columns[~all_clusters.dtypes.eq('object')]
            all_triggers = pandas.DataFrame(all_triggers, columns=all_clusters.columns,)
            all_triggers[non_string_cols] = all_triggers[non_string_cols].apply(pandas.to_numeric)

            # add some columns for usefulness purposes
            event_info = 'event_{0}_external_slide_{1}'.format(int(event_time), '_'.join(time_slide.astype(str).tolist()))
            processed_map_info = '{0}-{1}'.format(int(event_time), '_'.join(time_slide.astype(str).tolist()))
            all_triggers['event'] = event_info
            if args.job_type in _injection_analysis_types:
                all_triggers['waveform'] = waveform_name
                all_triggers['injection_scale'] = injection_scale
                all_triggers['injection_number'] = injection_number
                all_triggers = prep_data.window_cut(all_triggers,duration=1.0)
                all_triggers.to_hdf('./output/grb-{0}-{1}-{2}-{3}-{4}-triggers.h5'.format(args.job_type, int(event_time), waveform_name, injection_numbers[0], injection_numbers[-1]), key='/injections', append=True, index=False,)
            else:
                all_triggers.to_hdf('./output/grb-{0}-{1}-triggers.h5'.format(args.job_type, processed_map_info), key='/background', append=True, index=False,)       
