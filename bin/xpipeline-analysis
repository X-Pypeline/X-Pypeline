#!/usr/bin/env python

from xpipeline.core.xtimeseries import XTimeSeries
from xpipeline.core.xdetector import Detector
from xpipeline.core.xdetector import compute_antenna_patterns
from xpipeline.likelihood.xlikelihood import XLikelihoodMap
from xpipeline.core.xtimefrequencymap import XTimeFrequencyMapDict
from xpipeline.core.xtimefrequencymap import csc_XSparseTimeFrequencyMap
from xpipeline.waveform import xinjectsignal
from xpipeline.setuputils import log
from gwpy.table import EventTable
from gwpy.timeseries import TimeSeries
from gwpy.signal import filter_design
from functools import reduce

import pandas
import numpy
import argparse
import tables

_injection_analysis_types = ['onsource_injection', 'zero_noise_injection', 'offsource_injection']
_other_event_types = ['background', 'onsource']
_job_types = []
_job_types.extend(_injection_analysis_types)
_job_types.extend(_other_event_types)

def parse_commandline():
    parser = argparse.ArgumentParser(description="This executable processes "
                                                 "injection, off source "
                                                 "and on source trials "
                                                 "X-Pipeline triggered search jobs.")
    # Define groups of flags to make the help output ore useful
    required_args = parser.add_argument_group('required named arguments')
    optional_args = parser.add_argument_group('optional arguments')
    required_args.add_argument("-j", "--job-type", help=
                               """
                               Job Type options are {0}
                               """.format(_job_types),
                               required=True)
    required_args.add_argument("-p", "--parameter-file", help=
                               """
                               This file is usually generated by
                               xpipeline-workflow and is in the input
                               directory. Example:
                               input/parameters_off_source_0.txt
                               """,
                               required=True)
    required_args.add_argument("-e", "--event-numbers", help=
                               """
                               This indicates what background timeslide
                               and timestamp to use
                               The file that the event-number picks from
                               is usually listed in the params file and
                               in a two detector analysis has the form
                               1126259462.0 0 0
                               (timestamp, seconds to slide detector 1,
                               slide detector 2
                               """,
                               required=True, type=int, nargs='+')
    required_args.add_argument("-i", "--injection-numbers", help=
                               """
                               This indicates what injections from
                               the injection files you want to process
                               the injection file usually is in the input folder
                               and has the naming convention injection_"waveform_name".txt
                               Each row of the file has the form
                               gps_s, gps_ns, phi, theta, psi, gwb_type, gwb_params
                               1135227846 42243481 2.034325e+00 1.014036e+00 6.620518e-01 chirplet 1.0e-22~0.01~100~0~0~1
                               """,
                               type=int, nargs='+')

    args = parser.parse_args()

    if args.job_type not in _job_types:
        raise parser.error('You can only perform one of the following analyses '
                           '{0}'.format(_job_types))

    if args.job_type == 'injection' and (args.injection_numbers is None):
        raise parser.error('You must specify what injections you want '
                           'to perform if you are doing an injection analysis.')

    return args

args = parse_commandline()

logger = log.Logger('XPIPELINE: Analysis {0}'.format(args.job_type))

# Read parameters
parameters = dict(line.rstrip('\n').split(':', 1) for line in open(args.parameter_file))

applycalibcorrection = parameters['applycalibcorrection']

# How mcuh data are you analyzing (standard is 256s)
block_time = int(parameters['blocktime'])

# Determine channels and frames being used for the analysis
channel_names_file = parameters['channelFileName']
channel_names = [chan.rstrip('\n').split(' ')[0]
                 for chan in open(channel_names_file)]
frame_types = [chan.rstrip('\n').split(' ')[1]
               for chan in open(channel_names_file)]
frame_types = dict(zip(channel_names, frame_types))

ifos = [chan.split(':')[0] for chan in channel_names]

# Minimum and Maximum search frequency range
minimum_frequency = int(parameters['minimumfrequency'])
maximum_frequency = int(parameters['maximumfrequency'])

# Sample frequency
sample_frequency = float(parameters['samplefrequency'])

# What overlap are you using for our ffts (standard is 50%)
offset_fraction = float(parameters['offsetfraction'])

frame_file = parameters['frameCacheFile']
whitening_length = int(parameters['whiteningtime'])
transient_time = 4.0

if 'circtimeslidestep' in parameters.keys():
    slides = {}
    circ_time_slides = int(parameters['circtimeslidestep'])
    nchannels = len(channel_names)
    if nchannels == 1:
        slides[channel_names[0]] = numpy.zeros(1).astype(int)
    elif nchannels == 2:
        internal_slides = numpy.arange(0, (block_time - 2*transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
    elif nchannels == 3:
        internal_slides = numpy.arange(0, (block_time / 2 - transient_time -
                                           circ_time_slides),
                                       circ_time_slides).astype(int)
        slides[channel_names[0]] = numpy.zeros(internal_slides.size)
        slides[channel_names[1]] = internal_slides
        slides[channel_names[2]] = -internal_slides
    else:
        raise ValueError("The circular time slides are not "
                         "implemented for the requested "
                         "number of detectors: {0}".format(nchannels))
else:
    circ_time_slides = None
    internal_slides = numpy.zeros(1).astype(int)
    slides = {k: internal_slides for k in channel_names}

event_file = parameters['eventFileName']

on_source_end_offset = int(parameters['onsourceendoffset'])
on_source_begin_offset = int(parameters['onsourcebeginoffset'])

sky_positions = parameters['skyPositionList']
sky_positions = EventTable.read(sky_positions, format='ascii',
                                names=['theta', 'phi', 'probability', 'area'])

seed = int(parameters['seed'])
fft_lengths = [float(fft) for fft in parameters['analysistimes'].split(',')]
min_fftlength = min(fft_lengths)
likelihoods = parameters['likelihoodtype'].split(',')

numpy.random.seed(seed)
max_pixels = int(1./offset_fraction * 1./min_fftlength * block_time * (sample_frequency/2 * min_fftlength + 1) *len(ifos)* 0.01)

table_description = {
'dx' : tables.Float64Col(),
'dy' : tables.Float64Col(),
'x0' : tables.Float64Col(),
'y0' : tables.Float64Col(),
'shape' : tables.Int64Col(2),
'phi' : tables.Float64Col(),
'theta' : tables.Float64Col(),
'map_type' : tables.StringCol(20),
'ifo' : tables.StringCol(max([len(i) for i in channel_names])),
'x' : tables.Float64Col(shape=max_pixels),
'y' : tables.Float64Col(shape=max_pixels),
'energy' : tables.ComplexCol(itemsize=16, shape=max_pixels),
}

# presets
events = pandas.read_csv(event_file,header=None, delimiter=' ').values
events = events.astype(int)
event = events[args.event_numbers]
for event_time, time_slide in zip(event[:,0], event[:,1:]):
    start_time = event_time - block_time / 2;

    # likelihoods to analze
    time_slides = dict(zip(channel_names, time_slide))
    analysis_data = XTimeSeries.retrieve_data(event_time, block_time,
                                              channel_names, sample_frequency,
                                              frame_types=frame_types, time_slides=time_slides)

    # Are we doing injections and if so how many injection scales are we doing?
    if args.job_type in _injection_analysis_types:
        logger.info('Injecting a signal... Reading injection specific parameters')
        injection_file_name = parameters['injectionFileName']
        waveform_name = injection_file_name.split('_')[1].split('.')[0]
        injection_scales = [float(i) for i in parameters['injectionScales'].split(',')]
        injection_numbers = args.injection_numbers
        catalogdirectory = parameters['catalogdirectory']
    else:
        # If not doing injections there are no injections scales (similarly there
        # are no injection numbers to be done)
        injection_scales = [None]
        injection_numbers = [None]

    # Loop over injections
    for injection_number in injection_numbers:
        if args.job_type in _injection_analysis_types:
            logger.info('Creating injection number {0}'.format(injection_number))
            [injection_data, gps_s, gps_ns, phi, theta, psi] = xinjectsignal.xinjectsignal_fromfile(start_time=start_time, block_time=block_time, channels=channel_names, injection_file_name=injection_file_name, injection_number=injection_number, sample_rate=sample_frequency, catalogdirectory=catalogdirectory)

        # Loop over injection scales
        for injection_scale in injection_scales:
            if args.job_type in _injection_analysis_types:
                logger.info('Scaling amplitude by injection scale: {0}'.format(injection_scale))
                scaled_injection_data = XTimeSeries()
                for k, v in injection_data.items():
                    scaled_injection_data[k] = v * injection_scale

                if args.job_type == 'zero_noise_injection':
                    logger.info('Injecting into zero noise data')
                    zero_noise_data = XTimeSeries()
                    for channel in channel_names:
                        zero_noise_data.append({channel : TimeSeries(numpy.zeros(int(sample_frequency * block_time)),
                                                                   channel=channel,
                                                                   name=channel,
                                                                   dt=1./sample_frequency,
                                                                   t0 = start_time)})
                    zero_noise_injection = zero_noise_data.inject(scaled_injection_data)
                    data = analysis_data.copy()
                else:
                    logger.info('Injecting into data')
                    data = analysis_data.inject(scaled_injection_data)
            else:
                data = analysis_data

            # high pass filter the data
            data = data.highpass(minimum_frequency,)
            # Get the ASDs that will be used to whiten the data
            asds = data.asd(whitening_length)

            if args.job_type == 'zero_noise_injection':
                # use real asd to whiten data with no noise
                whitened_timeseries = zero_noise_injection.whiten(asds)
                #tmp = zero_noise_injection.plot()
                #ax = tmp.gca()
                #ax.set_xlim([gps_s + gps_ns * 10e-10-0.2, gps_s + gps_ns * 10e-10 +0.2])
                #tmp.savefig('/home/scoughlin/public_html/PhD/Project/LIGO/BURST/xpipeline/grb-dev/zero-noise-injection.png')
                #tmp = whitened_timeseries.plot()
                #ax = tmp.gca()
                #ax.set_xlim([gps_s + gps_ns * 10e-10-0.2, gps_s + gps_ns * 10e-10 +0.2])
                #tmp.savefig('/home/scoughlin/public_html/PhD/Project/LIGO/BURST/xpipeline/grb-dev/zero-noise-injection-whitened.png')


            else:
                whitened_timeseries = data.whiten(asds)
                #tmp = data.plot()
                #ax = tmp.gca()
                #ax.set_xlim([gps_s + gps_ns * 10e-10-0.2, gps_s + gps_ns * 10e-10 +0.2])
                #tmp.savefig('/home/scoughlin/public_html/PhD/Project/LIGO/BURST/xpipeline/grb-dev/injection.png')
                #tmp = whitened_timeseries.plot()
                #ax = tmp.gca()
                #ax.set_xlim([gps_s + gps_ns * 10e-10-0.2, gps_s + gps_ns * 10e-10 +0.2])
                #tmp.savefig('/home/scoughlin/public_html/PhD/Project/LIGO/BURST/xpipeline/grb-dev/injection-whitened.png')

            # Obtain detector specific information
            detectors = {}
            for det in ifos:
                detectors[det] = Detector(det)

            # Pre calculate time delays number of instruments-1 by number of sky positons
            time_delays = numpy.zeros((sky_positions['phi'].size, len(detectors)-1))
            phi = numpy.array(sky_positions['phi'])
            theta = numpy.array(sky_positions['theta'])


            for idx, idetector in enumerate(ifos[1::]):
                time_delays[:, idx] = detectors[ifos[0]].time_delay_from_earth_center_phi_theta(phi, theta) - detectors[idetector].time_delay_from_earth_center_phi_theta(phi, theta)

            for fft_length in fft_lengths:
                ####################
                # Create FFT grams #
                ####################

                # Make a spectrogram that contains phase information
                fft_grams_fixed = whitened_timeseries.fftgram(fft_length).crop_frequencies(minimum_frequency, maximum_frequency)

                # determine how many indices every pixel needs to move
                # with respect to the internal time slides
                final_time_bin_idx = fft_grams_fixed[channel_names[0]].shape[0]
                if circ_time_slides is not None:
                    slide_pixels = {k: v * 1./fft_length * 1./offset_fraction for k,v in slides.items()}
                else:
                    slide_pixels = {k: numpy.zeros(1) for k in channel_names}

                fft_grams = fft_grams_fixed.copy()
                for (sky_position, time_delay) in zip(sky_positions, time_delays):
                    phi = sky_position['phi']
                    theta = sky_position['theta']
                    antenna_patterns = compute_antenna_patterns(['H1', 'L1'], phi, theta,
                                                                antenna_patterns=['f_plus', 'f_cross'])
                    projected_asds = asds.project_onto_antenna_patterns(antenna_patterns,
                                                                        to_dominant_polarization_frame=True)

                    projected_asds = projected_asds.to_unit()

                    # NOTE we calculate everything from here to the loop over
                    # internal time slides for the non-slide maps
                    # this way when we find pizels for the slide maps
                    # if we undo the slide to the non-slide maps values
                    # then we do not need to recalculate say likelihood maps again
                    # and again

                    ####################################################
                    # Create TF maps and Likelihood Maps               #
                    ####################################################

                    # Make a spectrogram that contains phase information
                    fft_grams['L1:GDS-CALIB_STRAIN'] = fft_grams_fixed['L1:GDS-CALIB_STRAIN'].phaseshift(time_delay)
                    # Create Dominant Polariztion Frame projected TF map
                    projected_fftmaps = fft_grams.to_dominant_polarization_frame(projected_asds)

                    ####################################################
                    # Find loud pixels and down select maps            #
                    ####################################################
                    # Create spectrogram that contains info of the energy in each pixel
                    energy_maps = fft_grams.abs()

                    # Turn off the bottom 99 percent of pixels (i.e. set to 0)
                    energy_maps_zeroed = energy_maps.blackout_pixels(99)

                    # Calculate all time slides
                    tindex_findex_slided = {k : zip(numpy.mod(numpy.repeat(numpy.atleast_2d(energy_maps_zeroed[k].nonzero()[0]), len(v),0) + numpy.atleast_2d(v).T, final_time_bin_idx).astype(int), numpy.repeat(numpy.atleast_2d(energy_maps_zeroed[k].nonzero()[1]), len(v), 0))  for k,v in slides.items()}

                    # Make every TF pixels pair in all maps
                    all_maps_pixel_pairs = [zip(i[0], i[1]) for k,v in tindex_findex_slided.items() for i in v]
                    array_of_tf_pixels_per_slide_per_det = numpy.asarray(all_maps_pixel_pairs).reshape(-1, internal_slides.size,)

                    # all coherent tf pixels accross all maps
                    all_pixels = numpy.apply_along_axis(lambda x: reduce(lambda x, y: set(x).union(y), x.tolist()), 0, array_of_tf_pixels_per_slide_per_det)

                    for coherent_pixels, circular_time_slide in zip(all_pixels, internal_slides):

                        # make sure that the coherent time frequency pixels are sorted
                        coherent_pixels = sorted(coherent_pixels)

                        # extract the tindices and the findices
                        tindex, findex = zip(*coherent_pixels)
                        tindex = numpy.asarray(tindex)
                        findex = numpy.asarray(findex)

                        # extract the tindices and the findices
                        tindex = {k : tindex for k in channel_names}
                        findex = {k : findex for k in channel_names}

                        # Now that we have the pixels that are in each map
                        # let's reconstruct the sparse tf map
                        # Now with pixels from both detectors
                        surviving_pixels = {k : v.to_sparse(tindex, findex, phi=phi, theta=theta, map_type=k) for k, v in projected_fftmaps.items()}
                        surviving_pixels['excess_energy'] = fft_grams.to_sparse(tindex, findex, phi=phi, theta=theta)

                        for k, v in surviving_pixels.items():
                            processed_map_info = '{0}-{1}'.format(int(event_time), '_'.join(time_slide.astype(str).tolist()))
                            if args.job_type in _injection_analysis_types:
                                v.write(filename='./output/grb-{0}-{1}-{2}-{3}.h5'.format(args.job_type, int(event_time), injection_numbers[0], injection_numbers[-1]),
                                      path='/{0}/event_{1}_slide_{2}/waveform_{3}/injection_scale_{4}/injection_number_{5}'.format(args.job_type, int(event_time), '_'.join(time_slide.astype(str).tolist()), waveform_name, str(injection_scale).replace('.', 'd'), injection_number,),
                                      table_description=table_description)
                            else:
                                v.write(filename='./output/grb-{0}-{1}.h5'.format(args.job_type, processed_map_info),
                                      path='/{0}/event_{1}_external_slide_{2}/internal_slide_{3}'.format(args.job_type, int(event_time), '_'.join(time_slide.astype(str).tolist()), circular_time_slide),
                                      table_description=table_description)
