#!/usr/bin/env python

from xpipeline.core.xtimeseries import XTimeSeries
from xpipeline.core.xdetector import Detector
from xpipeline.core.xdetector import compute_antenna_patterns
from xpipeline.likelihood.xlikelihood import XLikelihoodMap
from xpipeline.core.xtimefrequencymap import XTimeFrequencyMapDict
from xpipeline.core.xtimefrequencymap import csc_XSparseTimeFrequencyMap
from xpipeline.waveform import xinjectsignal
from xpipeline.setuputils import log
from gwpy.table import EventTable

import pandas
import numpy
import argparse
import tables

def parse_commandline():
    parser = argparse.ArgumentParser(description="This executable processes "
                                                 "injection, off source "
                                                 "and on source trials "
                                                 "X-Pipeline triggered search jobs.")
    # Define groups of flags to make the help output ore useful
    required_args = parser.add_argument_group('required named arguments')
    required_args.add_argument("-j", "--job-type", help=
                               """
                               Are you analyzing an injection
                               or a background trial
                               """,
                               required=True)
    required_args.add_argument("-p", "--parameter-file", help=
                               """
                               This file is usually generated by
                               xpipeline-workflow and is in the input
                               directory. Example:
                               input/parameters_off_source_0.txt
                               """,
                               required=True)
    required_args.add_argument("-e", "--event-numbers", help=
                               """
                               This indicates what background timeslide
                               and timestamp to use
                               The file that the event-number picks from
                               is usually listed in the params file and
                               in a two detector analysis has the form
                               1126259462.0 0 0
                               (timestamp, seconds to slide detector 1,
                               slide detector 2
                               """,
                               required=True, type=int, nargs='+')
    required_args.add_argument("-i", "--injection-numbers", help=
                               """
                               This indicates what injections from
                               the injection files you want to process
                               the injection file usually is in the input folder
                               and has the naming convention injection_"waveform_name".txt
                               Each row of the file has the form
                               gps_s, gps_ns, phi, theta, psi, gwb_type, gwb_params
                               1135227846 42243481 2.034325e+00 1.014036e+00 6.620518e-01 chirplet 1.0e-22~0.01~100~0~0~1 
                               """,
                               type=int, nargs='+')


    args = parser.parse_args()

    if args.job_type not in ['injection', 'background']:
        raise parser.error('You can only perform an analysis on '
                           'an injected signal or a background job.')

    if args.job_type == 'injection' and (args.injection_numbers is None):
        raise parser.error('You must specify what injections you want '
                           'to perform if you are doing an injection analysis.')

    return args

args = parse_commandline()

logger = log.Logger('XPIPELINE: Analysis {0}'.format(args.job_type))

# Read parameters
parameters = dict(line.rstrip('\n').split(':', 1) for line in open(args.parameter_file))

applycalibcorrection = parameters['applycalibcorrection']

block_time = int(parameters['blocktime'])

minimum_frequency = int(parameters['minimumfrequency'])
maximum_frequency = int(parameters['maximumfrequency'])
offset_fraction = float(parameters['offsetfraction'])

frame_file = parameters['frameCacheFile']
whitening_length = int(parameters['whiteningtime'])
transient_time = 4.0

if 'circtimeslidestep' in parameters.keys():
    circ_time_slides = int(parameters['circtimeslidestep'])
else:
    circ_time_slides = None

event_file = parameters['eventFileName']

on_source_end_offset = int(parameters['onsourceendoffset'])
on_source_begin_offset = int(parameters['onsourcebeginoffset'])

sky_positions = parameters['skyPositionList']
sky_positions = EventTable.read(sky_positions, format='ascii',
                                names=['phi', 'theta', 'probability', 'area'])

seed = int(parameters['seed'])
fft_lengths = [float(fft) for fft in parameters['analysistimes'].split(',')]
min_fftlength = min(fft_lengths)
likelihoods = parameters['likelihoodtype'].split(',')

channel_names = parameters['channelFileName']
channel_names = [chan.rstrip('\n').split(' ')[0]
                 for chan in open(channel_names)]

ifos = [chan.split(':')[0] for chan in channel_names]

sample_frequency = float(parameters['samplefrequency'])

numpy.random.seed(seed)
max_pixels = int(1./offset_fraction * 1./min_fftlength * block_time * (sample_frequency/2 * min_fftlength + 1) *len(ifos)* 0.01)

table_description = {
'dx' : tables.Float64Col(1),
'dy' : tables.Float64Col(1),
'x0' : tables.Float64Col(1),
'y0' : tables.Float64Col(1),
'shape' : tables.Int64Col(2),
'phi' : tables.Float32Col(1),
'theta' : tables.Float32Col(1),
'map_type' : tables.StringCol(20),
'ifo' : tables.StringCol(max([len(i) for i in channel_names])),
'x' : tables.Float64Col(shape=max_pixels),
'y' : tables.Float64Col(shape=max_pixels),
'energy' : tables.ComplexCol(itemsize=16, shape=max_pixels),
}

# presets
events = pandas.read_csv(event_file,header=None, delimiter=' ').values
event = events[args.event_numbers]
for event_time, time_slide in zip(event[:,0], event[:,1:]):
    start_time = event_time - block_time / 2;

    # likelihoods to analze
    try:
        analysis_data = XTimeSeries.read('examples/Oct_10_2015_data.hdf5',)
    except:
        analysis_data = XTimeSeries.retrieve_data(event_time, block_time,
                                         channel_names, sample_frequency)

    # Are we doing injections and if so how many injection scales are we doing?
    if args.job_type == 'injection':
        logger.info('Injecting a signal... Reading injections pecific parameters')
        injection_file_name = parameters['injectionFileName']
        waveform_name = injection_file_name.split('_')[1].split('.')[0]
        injection_scales = [float(i) for i in parameters['injectionScales'].split(',')]
        injection_numbers = args.injection_numbers
        catalogdirectory = parameters['catalogdirectory']
    else:
    # If not doing injections there are no injections scales (similarly there
    # are no injection numbers to be done)
        injection_scales = []
        injection_numbers = []

    for injection_scale in injection_scales:
        for injection_number in injection_numbers:
            if args.job_type == 'injection':
                logger.info('Creating injections')
                [injection_data, gps_s, gps_ns, phi, theta, psi] = xinjectsignal.xinjectsignal_fromfile(start_time=start_time, block_time=block_time, channels=channel_names, injection_file_name=injection_file_name, injection_number=injection_number, sample_rate=sample_frequency, catalogdirectory=catalogdirectory)
                logger.info('Scaling amplitude by injection scale: {0}'.format(injection_scale))
                for k, v in injection_data.items():
                    injection_data[k] = v * injection_scale

                logger.info('Injecting into data')
                data = analysis_data.inject(injection_data)
            else:
                data = analysis_data.copy()

            # Get the ASDs that will be used to whiten the data
            asds = data.asd(whitening_length)

            # Whiten the timeseries
            whitened_timeseries = data.whiten(asds)

            # Obtain detector specific information
            detectors = {}
            for det in ifos:
                detectors[det] = Detector(det)

            # Pre calculate time delays number of instruments-1 by number of sky positons
            time_delays = numpy.zeros((sky_positions['phi'].size, len(detectors)-1))
            phi = numpy.array(sky_positions['phi'])
            theta = numpy.array(sky_positions['theta'])


            for idx, idetector in enumerate(ifos[1::]):
                time_delays[:, idx] = detectors[ifos[0]].time_delay_from_earth_center_phi_theta(phi, theta) - detectors[idetector].time_delay_from_earth_center_phi_theta(phi, theta)

            for fft_length in fft_lengths:
                ####################
                # Create FFT grams #
                ####################

                # Make a spectrogram that contains phase information
                fft_grams_fixed = whitened_timeseries.fftgram(fft_length)
                fft_grams = fft_grams_fixed.copy()
                for (sky_position, time_delay) in zip(sky_positions, time_delays):
                    phi = sky_position['phi']
                    theta = sky_position['theta']
                    antenna_patterns = compute_antenna_patterns(['H1', 'L1'], phi, theta,
                                                                antenna_patterns=['f_plus', 'f_cross'])
                    projected_asds = asds.project_onto_antenna_patterns(antenna_patterns,
                                                                            to_dominant_polarization_frame=True)

                    # NOTE we calculate everything from here to the loop over
                    # internal time slides for the non-slide maps
                    # this way when we find pizels for the slide maps
                    # if we undo the slide to the non-slide maps values
                    # then we do not need to recalculate say likelihood maps again
                    # and again

                    ####################################################
                    # Create TF maps and Likelihood Maps               #
                    ####################################################

                    # Make a spectrogram that contains phase information
                    fft_grams['L1:GDS-CALIB_STRAIN'] = fft_grams_fixed['L1:GDS-CALIB_STRAIN'].phaseshift(time_delay)
                    # Create Dominant Polariztion Frame projected TF map
                    projected_fftmaps = fft_grams.to_dominant_polarization_frame(projected_asds)

                    ####################################################
                    # Find loud pixels and down select maps            #
                    ####################################################
                    # Create spectrogram that contains info of the energy in each pixel
                    energy_maps = fft_grams.abs()

                    # Turn off the bottom 99 percent of pixels (i.e. set to 0)
                    energy_maps_zeroed = energy_maps.blackout_pixels(99)

                    # Add maps together and then back extract the t-f indices
                    # of the pixels, these will represent all t-f pixels used
                    # in our analysis and new sparse tf maps will need
                    # to be made for the individual ifos
                    coh_energy_maps_zeroed = energy_maps_zeroed.to_coherent()

                    # find nonzero pixels indicies
                    tf_indices = coh_energy_maps_zeroed.nonzero()

                    # If we are doing internal time slides we should register the time bins
                    # as these are the only things that will be shifted for any
                    # detectors
                    tindex = {k : tf_indices[0] for k in energy_maps}
                    findex = {k : tf_indices[1] for k in energy_maps}

                    # Now that we have the pixels that are in each map
                    # let's reconstruct the sparse tf map
                    # Now with pixels from both detectors
                    surviving_pixels = {k : v.to_sparse(tindex, findex) for k, v in projected_fftmaps.items()}
                    surviving_pixels['excess_energy'] = fft_grams.to_sparse(tindex, findex, phi=phi, theta=theta)
                    for k, v in surviving_pixels.items():
                        if args.job_type == 'injection':
                            v.write(filename='sn-analysis.h5',
                                  path='/{0}/event_{1}_slide_{2}/waveform_{3}/injection_scale_{4}/injection_number_{5}'.format(args.job_type, event_time, '_'.join(time_slide.astype(str).tolist()), waveform_name, str(injection_scale).replace('.', 'd'), injection_number,),
                                  table_description=table_description)
                        else:
                            v.write(filename='sn-analysis.h5',
                                  path='/{0}/event_{1}_slide_{2}/'.format(args.job_type, event_time, '_'.join(time_slide.astype(str).tolist()),),
                                  table_description=table_description)
