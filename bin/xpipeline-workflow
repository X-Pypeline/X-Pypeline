#!/usr/bin/env python

# -------------------------------------------------------------------------
#      Setup.
# -------------------------------------------------------------------------

# ---- Import standard modules to the python path.
import sys, os, shutil, math, random, copy, getopt, re, string, time
import glob, operator, argparse, socket, socket, uuid, pdb
import numpy as np
#from glue import pipeline
from glue import datafind
from numpy import loadtxt

from six.moves.configparser import ConfigParser
from six.moves import reduce

import lal

import json

from xpipeline import xcondor, xparseoutputtype
from xpipeline.skyutils import skyutils
from xpipeline.io import utils as ioutils
from xpipeline.setuputils import utils as setuputils
from xpipeline.setuputils import xnetworkselection, log

from gwpy.detector import Channel
from gwpy.segments import SegmentList, Segment

def parse_commandline():
    parser = argparse.ArgumentParser(description="Script for setting up \
                                                 X-Pipeline triggered search jobs.")
    # Define groups of flags to make the help output ore useful
    requiredNamed = parser.add_argument_group('required named arguments')
    requiredNamed.add_argument("-s", "--search-type", help=
                               """
                               Search Type you would like to perform
                               """,
                               required=True)
    requiredNamed.add_argument("-p", "--params-file", help=
                               """
                               Parameters (.ini) file
                               """,
                               required=True)
    requiredNamed.add_argument("-n", "--grb-name", help=
                               """
                               Name of GRB, e.g., GRB070201
                               """,
                               required=True)
    requiredNamed.add_argument("-g", "--grb-time", help=
                               """
                               GRB trigger time (GPS seconds)
                               """,
                               required=True, dest='trigger_time', type=float)
    requiredNamed.add_argument("-r", "--right-ascension", help=
                               """
                               Right ascension of GRB (degrees, from 0 to 360)
                               """,
                               required=True, dest='ra', type=float)
    requiredNamed.add_argument("-d", "--declination", help=
                               """
                               Declination of GRB (degrees, from -90 to 90)
                               """,
                               required=True, dest='decl', type=float)
    requiredNamed.add_argument("-i", "--detector", help=
                               """
                               Add detector to the network
                               """,
                               required=True, nargs='+')
    parser.add_argument("--FAR", help=
                        """
                        To be used with the --search-type=allsky argument.
                        This controls the number of background trials/jobs
                        such that the desired FAR is achieved roughly exactly.
                        Will warn and error if not enough coincident background
                        to achieve desired FAR
                        """,
                        type=float)
    parser.add_argument("--FAP", help=
                        """
                        To be used with the --search-type=sn or grb argument.
                        This controls the number of background trials/jobs
                        such that the desired FAP is achieved roughly exactly.
                        Will warn and error if not enough coincident background
                        to achieve desired FAP
                        """,
                        type=float)
    parser.add_argument("-e", "--sky-pos-err", help=
                        """
                        1-sigma uncertainty in sky position of GRB
			(degrees) or file name of sky position grid to be used
			with -t file option.
                        """,
                        type=float)
    parser.add_argument("-t", "--grid-type", help=
                        """
                        String. Determines what shape of sky position
			grid will be generated.  Recognized values are
			'circular' (2-d grids constructed from concentric
			'healpix' (2-d grids constructed using
			the healpix algorithm), 'line' (1-d arc
			grid), and 'file' (user generated grid of point, given
			with -e option). 'timedelay' (1-D arc that creates
			fixed time delay steps between base line of Dets
			Default 'circular'.
                        """,
                        default='circular')
    parser.add_argument("--right-ascension2", help=
                        """
                        Right ascension of center of second trigger
                              error circle (degrees, from 0 to 360).  Intended
                              for ANTARES HEN analysis.
                        """,
                        type=float, dest='ra2')
    parser.add_argument("--declination2", help=
                        """
                        Declination of center of second trigger\
                        error circle (degrees, from -90 to 90).  Intended\
                        for ANTARES HEN analysis.
                        """,
                        type=float, dest='decl2')
    parser.add_argument("--sky-pos-err2", help=
                        """
                        1-sigma uncertainty in sky position of second\
                        trigger error circle (degrees).
                        """,
                        type=float)
    parser.add_argument("-f", "--grid-sim-file", help=
                       """
                       File which contains (R.A.,Dec) coordinates
		       of simulated source positions.  To be used with
		       '--grid-type file' option.
                       """,
                       default='')
    parser.add_argument("--network-selection", help=
                        """
                        If this flag is set we select the appropriate
			network of IFOs from the set specified using
			the --detector option using our data-quality
			selection criteria; see <https://www.lsc-group.
			phys.uwm.edu/twiki/bin/view/Bursts/
			S5VSR1GRBNetworksV2pt5DQ>
                        """,
                        action='store_true', default=False)
    parser.add_argument("--injdistrib", help=
                        """
                        Tilde delimited string of parameters describin\
			the injection distribution in the first circle
			  Formats are:
			    1 parameter: 1-sigma containment of a fisher distribution
			    3 parameters: lognormal distribution in degrees
			    4 parameters: fisher distribution of statistical error and
			    core + tail fisher distribution of systematic error.
			    [stat_sigma sys_core_sigma fraction_core sys_tail_sigma]
			    all sigma are in degrees
                        """)
    parser.add_argument("--injdistrib2", help=
                        """
                        Same as --injdistrib but for second error circle.
                        """)
    parser.add_argument("--elognormal", help="Same as --injdistrib")
    parser.add_argument("--elognormal2", help="Same as --injdistrib2")
    parser.add_argument("--priority", help=
                        """
                        Integer specifying the priority of condor jobs.
			Default value is 0, higher priority jobs are
			submitted first to the cluster.
                        """,
                        default="0", dest='condorPriority')
    parser.add_argument("--end-offset", help=
                        """
                        Specify that the end offset of the onsource window should be
			greater or equal <offset>. The maximum of the end offset in the ini
			file and <offset> is used.
                        """,
                        type=float, default=0, dest='manualEndOffset')
    parser.add_argument("-m", "--mdc-path", help=
                        """
                        Path to mdc parent dir
                        If this option is used then the mdc log files
		        for each GRB will be copied from
		        <mdc-path>/GRB_<grb-name>/<waveform>/logs
		        into the GRBs /input dir and renamed according
		        to X-Pipeline conventions.
                        """)
    parser.add_argument("--disable-fast-injections", help=
                        """
                        If set disables the fast processing of injections
			time frequency map produced only for small window
			around injection time
                        """,
                        dest='disableFastInjections', action='store_true', default=False)
    parser.add_argument("-c", "--catalogdir", help=
                        """
                        Specify location of waveform catalog files for
			astrophysical injections (e.g., supernovae)
                        """,
                        dest='catalog_dir')
    parser.add_argument("--big-mem", help=
                       """
                       Integer specifying the minimal memory requirement
			      for condor jobs in MB.
                       """,
                       type=int, dest='minimalMem')
    parser.add_argument("--use-merging-cuts", help=
                        """
                        Specify the location of a post-processing
			directory from which coherent cuts
			should be read and applied to triggers
			at the the trigger collection stage
                        """,
                        dest='mergingCutsPath')
    parser.add_argument("--reuse-inj", help=
                        """
                        If --use-merging-cuts option also set will
			reuse the injection from the previous cut-tuning
			run. Use with caution no check is performed on
			consistency between provided and needed injections.
                        """,
                        dest='reUseInj', action='store_true', default=False)
    parser.add_argument("--xtmva", help=
                        """
                        Perform XTMVA analysis.
                        """,
                        dest='xtmvaFlag', action='store_true', default=False)
    parser.add_argument("--off-source-inj", help=
                        """
                        If set perform injections into the off-source
	                instead of the on-source region.
                        """,
                        dest='offSourceInj', action='store_true', default=False)
    parser.add_argument("--long-inj", help=
                        """
                        If specified, then when making injection dags a
			buffer will be used in comparing injection peak
			time to block interval to determine if an
			injection should be processed by multiple blocks.
			Works only for waveform sets with names beginning
			with adi-a, adi-b, adi-c, adi-d, adi-e, ebbh-a,
			ebbh-d, ebbh-e, mva. See code for details.
                        """,
                        dest='longInjections', action='store_true', default=False)
    parser.add_argument("--smart-cluster", help=
                        """
                        This will produce an extra set of DAGs that will run
			the smart clustering for large production analyses.
                        """,
                        dest='smartCluster', action='store_true', default=False)
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-v", "--verbose", action="store_true")
    group.add_argument("-q", "--quiet", action="store_true")
    args = parser.parse_args()

    return args

# -------------------------------------------------------------------------
#      Parse the command line options.
# -------------------------------------------------------------------------
args = parse_commandline()

# Set command line arguments to variables to be used later in the code
# Also based on command line arguments set some other defaults
reUseInj = args.reUseInj
mergingCutsPath = args.mergingCutsPath
grid_type = args.grid_type
grb_name = args.grb_name
params_file = args.params_file
trigger_time = round(args.trigger_time)
ra = args.ra
decl = args.decl
sky_pos_err = args.sky_pos_err
injdistrib = args.injdistrib
ra2 = args.ra2
decl2 = args.decl2
sky_pos_err2 = args.sky_pos_err2
injdistrib2 = args.injdistrib2
detector = args.detector
network_selection = args.network_selection
mdc_path = args.mdc_path
condorPriority = args.condorPriority
xtmvaFlag = args.xtmvaFlag
manualEndOffset = args.manualEndOffset
longInjections = args.longInjections
disableFastInjections = args.disableFastInjections
catalog_dir = args.catalog_dir
offSourceInj = args.offSourceInj
grid_sim_file = args.grid_sim_file
smartCluster = args.smartCluster
minimalMem = args.minimalMem
FAR = args.FAR
FAP = args.FAP

logger = log.Logger('X-Pypline')

if ra2 and decl2 and sky_pos_err2:
    raise ValueError('New setup script no longer supports supplying two sky locations due '
                     'the amount of extra lines of code it requires.')


#
#    Validating inputs
#

for iDet in detector:
    if not iDet in lal.cached_detector_by_prefix.keys():
        raise ValueError('One of the detectors '
                        'supplied at command line is not '
                        'a valid detector. Valid detectors '
                        'include {0}'.format(
                        lal.cached_detector_by_prefix.keys()))

#
#    Naming defaults
#

# ---- grb_name should have format e.g., "GRB070201"
#      we append GRB prefix if missing unless we are analysing a
#      MOCK GRB.
if not(grb_name.startswith('MOCK')) and not(grb_name.startswith('GRB')):
    grb_name = 'GRB' + grb_name

# ---- if grb_name begins with MOCK then we are doing a MOCK analysis.
if grb_name.startswith('MOCK'):
    mockAnalysis = 1
else:
    mockAnalysis = 0


# -------------------------------------------------------------------------
#      Test to see if we are running on Atlas.
# -------------------------------------------------------------------------

# ---- Get partial hostname.
hostname = socket.gethostname()

# ---- Get full hostame.
fullhostname = socket.getfqdn()

# ---- Check hostname and set flag if necessary.
if 'atlas' in fullhostname:
    atlasFlag = 1
elif 'h2' in hostname:
    atlasFlag = 1
elif 'coma' in fullhostname:
    atlasFlag = 1
else:
    atlasFlag = 0


# -------------------------------------------------------------------------
#      Status message.  Report all supplied arguments.
# -------------------------------------------------------------------------

logger.info("####################################################")
if args.search_type == 'grb':
    logger.info("#              X-GRB Search Pipeline               #")
if args.search_type == 'sn':
    logger.info("#              X-SN Search Pipeline               #")
if args.search_type == 'allsky':
    logger.info("#              X-SPHRAD Search Pipeline               #")
logger.info("####################################################")
logger.info("\n")
logger.info("Parsed input arguments:")
logger.info("\n")
logger.info("     parameters file: {0}".format(params_file))
logger.info("        trigger time: {0}".format(trigger_time))
logger.info("        trigger name: {0}".format(grb_name))
logger.info("     right ascension: {0}".format(ra))
logger.info("         declination: {0}".format(decl))
logger.info("         sky pos err: {0}".format(sky_pos_err))
if injdistrib:
    logger.info("          injdistrib: {0}".format(injdistrib))
logger.info("    detector network: {0}".format(detector))
if network_selection:
    logger.info("   network selection: automatic")
logger.info("            mdc path: {0}".format(mdc_path))
logger.info("           grid type: {0}".format(grid_type))
if grid_type == 'file':
    logger.info("       grid sim file: {0}".format(grid_sim_file))
logger.info("     condor priority: {0}".format(condorPriority))
if mergingCutsPath:
    logger.info("   merging cuts from: {0}".format(mergingCutsPat))
if atlasFlag:
    logger.info("    running on atlas: yes")
if xtmvaFlag:
    logger.info("       running XTMVA: yes")
logger.info("\n")

# ---- Write ASCII file holding setUpJobs command.

summary_file = 'grb_summary.txt'
# ---- Append to summary file.
sfile = open(summary_file,'a')
sfile.write('\t'.join(['\nname','gps','ra','dec','network','numSky','numBG','analyse','\n']))
sfile.write('\t'.join([grb_name,str(trigger_time),str(ra),str(decl)]))
sfile.close()

# ---- We will record the command line arguments to setUpJobs in a file called
#      grb.param.
#      This file is used by xgrbwebpage.py which expects the short form of
#      the options to have been used
command_string = 'setUpJobs '
with open('setUpJobs.param', 'w') as f:
    f.write(command_string + ' '.join(sys.argv[1:]))

# -------------------------------------------------------------------------
#      Preparatory.
# -------------------------------------------------------------------------

# ---- Generate unique id tag.
uuidtag = str(uuid.uuid4()).replace('-','')

# ---- Record the current working directory in a string.
cwdstr = "."

# ---- Make directory to store text files (segment lists, parameter
#      files, etc.) that will be input to X-Pipeline.  This is done
#      lsto minimize clutter in the working directory.
try: os.mkdir( 'input' )
except: pass  # -- Kludge: should probably fail with error message.

# ---- Find files which define merging cuts
mergingCutsString = ""
if mergingCutsPath:
    preCutFile = glob.glob(mergingCutsPath + '/*xmake_args_tuned_pre.txt')
    cutFile = glob.glob(mergingCutsPath + '/*xmake_args_tuned.txt')
    origResults = glob.glob(mergingCutsPath + '/*closedbox.mat')
    mergingCutsString = cutFile[0] + " " + origResults[0]
    if preCutFile:
        mergingCutsString = mergingCutsString + " " + preCutFile[0]


# -------------------------------------------------------------------------
#      Read configuration file.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("Parsing parameters (ini) file ...")

# ---- Check the params_file exists
if not os.path.isfile(params_file):
    logger.error("Non existent parameter file: {0}".format(params_file))
    raise ValueError("Misspelled or non-existent ini file")

# Set some ini file defaults
doAsymmetricBackground    = 0
backgroundAsymmetryFactor = 0.5

# ---- Create configuration-file-parser object and read parameters file.
cp = ConfigParser()
cp.read(params_file)

if cp.has_option('tags','version') :
    ini_version = cp.get('tags','version')
    logger.info("Parameter file CVS tag: {0}".format(ini_version))

# ---- Check for a seed value for matlab's random number generator.
try:
    seed = int(cp.get('parameters','seed'))
except:
    seed = 931316785
    logger.warning("Warning: No seed specified in configuration file.")
    logger.warning("         seed will be set to: {0}".format(seed))

# ---- Matlab seed can take values between 0 and 2^31-2.
if (seed > pow(2,31)-2) or (seed < 0):
    logger.error("Seed must have value between 0 and 2^31-2")

# ---- NOTE: The following reading of variables can be split up and
#      moved into the relevant sections of the script where the
#      variables are actually used.

# ---- Read needed variables from [parameters] and [background] sections.
background_period = cp.getint('background','backgroundPeriod')

if cp.has_option('parameters','doAsymmetricBackground') :
    doAsymmetricBackground   = cp.getint('parameters','doAsymmetricBackground')

if cp.has_option('parameters','backgroundAsymmetryFactor') :
    backgroundAsymmetryFactor = cp.getfloat('parameters', 'backgroundAsymmetryFactor')

blockTime         =  cp.getint('parameters','blockTime')
whiteningTime     =  cp.getfloat('parameters','whiteningTime')
transientTime     =  4 * whiteningTime
onSourceBeginOffset = cp.getint('parameters','onSourceBeginOffset')
onSourceEndOffset   = cp.getint('parameters','onSourceEndOffset')
onSourceEndOffset   = max(onSourceEndOffset,manualEndOffset)
onSourceTimeLength = onSourceEndOffset - onSourceBeginOffset
outputType = cp.get('parameters','outputType')
xparseoutputtype.parse(outputType,cp)
jobsPerWindow = int(math.ceil(float(onSourceTimeLength)/float(blockTime-2*transientTime)))
onSourceWindowLength=2*transientTime+jobsPerWindow*(blockTime-2*transientTime)
minimumFrequency = cp.getint('parameters','minimumFrequency')
maximumFrequency = cp.getint('parameters','maximumFrequency')

channelListLine   = cp.get('input','channelList')
channelList       = channelListLine.split(',')
frameTypeListLine = cp.get('input','frameTypeList')
frameTypeList     = frameTypeListLine.split(',')
detectorListLine  = cp.get('input','detectorList')
detectorList      = detectorListLine.split(',')

# ---- Interval of data to be analysed.
# ---- Allow for asymmetric background analyses, if needed by the online analysis
# ---- Flag to be read from ini file
if args.search_type == 'grb':
    if doAsymmetricBackground == 0:
        logger.info("Running analysis with symmetric background")
        start_time = int(trigger_time - background_period / 2)
        end_time = int(trigger_time + background_period / 2)
    # ---- introduce fractional background asymmetry
    else:
        if backgroundAsymmetryFactor < 1.0:
            logger.warning("Running analysis with asymmetric background")
            start_time = int(trigger_time - background_period*backgroundAsymmetryFactor)
            end_time = int(trigger_time + background_period*(1.0 - backgroundAsymmetryFactor))
        else:
            logger.error( "Running analysis with asymmetric background but with wrong fractional asymmetry. backgroundAsymmetryFactor should be less than 1.0")
elif args.search_type in ['sn', 'allsky']:
    start_time = int(trigger_time + onSourceBeginOffset - transientTime)
    end_time = int(trigger_time + onSourceEndOffset + transientTime)

duration = int(end_time - start_time)


# ---- We will copy all ifo data and mdc frame caches to the location
#      stored in frameCacheAll.
frameCacheAll = 'input/framecache.txt'

# ---- Check for frame cache for real ifo data
try:
    dataFrameCache = cp.get('input','frameCacheFile')
except:
    logger.warning("Warning: No frameCacheFile file specified in "
        "[input] section of configuration file.")
    logger.warning("        A frameCache for the ifo data file will be "
        "generated automatically.")
    dataFrameCache = None

# ---- Check for a seed value for matlab's random number generator.
try:
    seed = int(cp.get('parameters','seed'))
except:
    seed = 931316785
    logger.warning("Warning: No seed specified in configuration file.")
    logger.warning("         seed will be set to: {0}".format(seed))

# ---- Matlab seed can take values between 0 and 2^31-2.
if (seed > pow(2,31)-2) or (seed < 0):
    logger.error("seed must have value between 0 and 2^31-2")

# ---- Status message.
logger.info("... finished parsing parameters (ini) file.")

# -------------------------------------------------------------------------
#      Validate list of detectors given.
# -------------------------------------------------------------------------

# ---- KLUDGE:TODO: Move this below automatic network selection so that we
#      only perform this once???

# ---- Status message.
logger.info("Comparing requested network to list of known detectors ...")

# ---- For each detector specified with the --detector option, compare to
#      the list of known detectors from the .ini file.  Keep only the requested
#      detectors and corresponding channel name and frame type.
#      KLUDGE:TODO: Should exit with error message if any of the detectors
#      is not recognized.
# ---- Indices of the detectors requested for this analysis.
keepIndex = []
for i in range(0,len(detector)) :
    for ii in range(0,len(detectorList)) :
        if detector[i] == detectorList[ii] :
            keepIndex.append(ii)
# ---- Sort indices so that order matches that used in ini file.
keepIndex.sort()
# ---- We now have a list of the indices of the detectors requested for the
#      analysis.  Keep only these.  Note that we over-write the 'detector'
#      list because we want to make sure the order matches the channel and
#      frameType lists.
detector  = []
channel   = []
frameType = []
for jj in range(0,len(keepIndex)):
    detector.append(detectorList[keepIndex[jj]])
    channel.append(channelList[keepIndex[jj]])
    frameType.append(frameTypeList[keepIndex[jj]])

# ---- Status message.
logger.info("... finished validating network.          ")

logger.info("... checking if framecache exists ...     ")
# ---- If dataFrameCache is specified and exists we will add it to frameCacheAll
if dataFrameCache:
    logger.info("... framecache does exists ...     ")
    # ---- check the frameCache specified actually exists
    if not os.path.isfile(dataFrameCache):
        logger.error("non existant framecache: {0}".format(dataFrameCache))

    # ---- if the specified mdc frame cache exists then concat it other frame caches
    command = 'cat ' + dataFrameCache + '  >> ' + frameCacheAll
    os.system(command)
else:
    # ---- Status message.
    logger.info("... framecache does NOT exists ...     ")
    logger.info("Writing framecache file for ifo data...")
    f = open(frameCacheAll, 'w')
    f.close()
    connection = datafind.GWDataFindHTTPConnection()
    for (iframetype, idet) in zip(frameType, detector):
        logger.info("Writing framecache file for detector {0} and frameType {1}...".format(idet, iframetype))
        cache = connection.find_frame_urls(idet.strip('1')[0], iframetype, start_time, end_time, urltype='file')
        cache.tofile(open(frameCacheAll, 'a+'))

    # ---- Status message.
    logger.info("... finished writing framecache file for ifo data")


# -------------------------------------------------------------------------
#      Retrieve single-IFO segment lists for analysis period.
# -------------------------------------------------------------------------

# Write our analysis duration as a gwpy segment list
analysis_segment = SegmentList([Segment(start_time, end_time)])

# ---- Prepare storage for full segment lists.
full_segment_list = []

if cp.has_option('parameters','makeSimulatedNoise'):
    # ---- Prepare storage for lists of analysis and veto segment filenames.
    analysis_seg_files = setuputils.validate_segments(detector, start_time, end_time, cp, trigger_time)
else:
    # ---- Prepare storage for lists of analysis and veto segment filenames.
    analysis_seg_files = setuputils.validate_segments(detector, start_time, end_time, cp)

veto_seg_files = setuputils.validate_vetos(detector, start_time, end_time, cp)

# ---- Read in segment_lists to ScienceData object.
for (iFile, ifo) in zip(analysis_seg_files, detector):

    # ---- Read full segment list into gwpy
    #      Throw away science segment shorter than the
    #      blockTime value read from the configuration file.
    logger.info('Reading segment list from file {0}'.format(iFile))
    full_segment = SegmentList.read(iFile)
    logger.info('... finished reading segment list.')
    logger.info('Checking for segments less than {0}.'.format(blockTime))
    full_segment = SegmentList([seg for seg in full_segment if abs(seg) > blockTime])
    logger.info('Done')

    # ---- Finally, append segment for this detector to the full list.
    full_segment_list.append(full_segment)

# -------------------------------------------------------------------------
#      Choose detector network based on on-source data quality.
# -------------------------------------------------------------------------

# ---- Pulled the segment lists stuff outside of the "if network_selection"
#      statement since we need these to test the off-source segments.
# ---- Get cat1 and cat24 segment files for all ifos used.
#      Use 'None' when no file exists.
all_detectors = lal.cached_detector_by_prefix.keys()
cat1_segment_file_list = {}
cat24_segment_file_list = {}
for ifo in all_detectors:
    try:
        # ----- Did we consider current ifo.
        idx = detector.index(ifo)
        cat1_segment_file_list[ifo] = analysis_seg_files[idx]
        cat24_segment_file_list[ifo] = veto_seg_files[idx]
    except (ValueError):
        cat1_segment_file_list[ifo] = "None"
        cat24_segment_file_list[ifo] = "None"

if network_selection and blockTime < 256:
    logger.error( "Network selection does not currently "
        "work for blockTimes < 256s \n")

if network_selection :
    logger.info("\nSelecting ifo network from ", detector)

    # ---- Write file containing GRB trigger time.
    ftriggertime=open('input/trigger_time.txt', 'w')
    ftriggertime.write("%f\n"%trigger_time)
    ftriggertime.close()

    # ---- Write file containing time offsets for on-source.
    ftimeoffsets=open('input/time_offsets.txt', 'w')
    for ifo in detector:
        ftimeoffsets.write("0 ")
    ftimeoffsets.close()

    network_outputFile = 'input/xnetworkselection_onsource.txt'

    channel, frameType, full_segment_list, detector = xnetworkselection.main(
                               detector, "input/trigger_time.txt",
                               "input/time_offsets.txt",
                               cat1_segment_file_list,
                               cat24_segment_file_list,
                               network_outputFile, onSourceBeginOffset,
                               onSourceEndOffset,
                               transientTime,
                               onSourceWindowLength)


logger.info('Based on valid network creating Tilde string useful for reading in MATLAB...')
# -----------------------------------------------------------------------------
#               Construct tilde-separated list of sites
# -----------------------------------------------------------------------------

# ---- Initialise tilde-separated list of sites.
siteStrTilde = ''
for iDet in range(0,len(detector)):
   # ---- Get name of current site from current (virtual) detector.
   siteTemp = detector[iDet][0]
   # ---- Add current site to list only if does not already appear in it.
   if siteStrTilde.count(siteTemp)==0:
      siteStrTilde = '~'.join([siteStrTilde,siteTemp])

# ---- Remove preceding tilde.
siteStrTilde = siteStrTilde[1:len(siteStrTilde)]

# ---- Write string listing detectors
detectorStr = ''
detectorStrTilde = ''
for ifo in detector:
    detectorStr = detectorStr + ifo
    detectorStrTilde = detectorStrTilde + ifo + '~'
detectorStrTilde = detectorStrTilde[0:len(detectorStrTilde)-1]

# ---- Append to summary file.
sfile = open(summary_file,'a')
sfile.write('\t' + detectorStr)
sfile.close()

# ---- Real frame type, for reading frames.
frameTypeStrTilde = ''
for frameTypeName in frameType:
  frameTypeStrTilde = frameTypeStrTilde + frameTypeName + '~'
frameTypeStrTilde = frameTypeStrTilde[0:len(frameTypeStrTilde)-1]

# -------------------------------------------------------------------------
#      Write channel file.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("Writing Matlab-formatted channel file ...      ")

# ---- For each detector, write the corresponding channel name, frame type,
#      and virtual channel name to the channels file to be read by xdetection.
f=open('input/channels.txt', 'w')
for i in range(0,len(detector)) :
    f.write(detector[i] + ':' + channel[i] + ' ' + frameType[i] + ' ' + detector[i] + ':' + channel[i] + '\n')
f.close()

# ---- Status message.
logger.info("... finished writing channel file.        ")


logger.info('Based on valid network selecting ini file params, '
            'such as lagType and likelihoodType.')

# -------------------------------------------------------------------------
#      Choose appropriate likelihoods and lags for this network.
# -------------------------------------------------------------------------

if len(detector) ==0:
    logger.error("No ifos in network!!!")

elif len(detector) ==1:
    logger.info("One ifo in network: {0}".format(detector))
    lagType = []
    likelihoodType = "likelihoodType_1det1site"

elif len(detector) ==2:
    logger.info("Two misaligned ifos in network: {0}".format(detector))
    lagType = "lags_2det2site"
    likelihoodType = "likelihoodType_2det2site"

elif len(detector) ==3:
    logger.info("Three ifos at three sites: {0}".format(detector))
    lagType = "lags_3det3site"
    likelihoodType = "likelihoodType_3det3site"

elif len(detector) ==4:
    logger.info("Four ifos at four sites: {0}".format(detector))
    lagType = "lags_4det4site"
    likelihoodType = "likelihoodType_4det4site"

elif len(detector) ==5:
    logger.info("Five ifos at five sites: {0}".format(detector))
    lagType = "lags_5det5site"
    likelihoodType = "likelihoodType_5det5site"

# -----------------------------------------------------------------------------
#         Having figured out network read in appropriate lag file.
# -----------------------------------------------------------------------------

# ---- We only need to read in a lag file if our network has 2 or more ifos.
if lagType:
    try:
        lagFile =  cp.get('background',lagType)
    except:
        logger.warning("No lagFile specified in configuration file.")
        logger.warning("No time lag jobs will be made.")
        lagFile =  None

    if lagFile:
        if not os.path.isfile(lagFile):
            logger.error("Error: non existant lag file: {0}".format(lagFile))
else:
    lagFile = None

logger.info("Using lag file: {0}".format(lagFile))


# -----------------------------------------------------------------------------
#         Having figured out network read in appropriate likelihood types.
# -----------------------------------------------------------------------------

try:
   likelihoodTypeStr =  cp.get('parameters',likelihoodType)
except:
   logger.error( "Error: required likelihoodType not specified in "
      "configuration file.")

logger.info("Using likelihoods: {0}".format(likelihoodTypeStr))

# -------------------------------------------------------------------------
#    Make coincidence segment list for on-source, zero-lag segment.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("Writing on-source event file ...          ")

# Here we calculate and create a setgment of the actual on source window of the analysis. This is not necessarily the same as the size of the onsource coincident segment which relates to the requested blockTime.
on_source_start_time = int(trigger_time + onSourceBeginOffset - transientTime)
on_source_end_time = int(trigger_time + onSourceEndOffset + transientTime)
on_source_window = SegmentList([Segment(on_source_start_time, on_source_end_time)])

# check that on-source segment overlaps with science segments
for seg in full_segment_list:
    if not seg.intersects(on_source_window):
        logger.error("Error: on-source period is not a coincidence segment "
                     "of the specified network."
                     "on source period: {0} {1} "
                     "coincidence period: {2} {3}".format(on_source_window.start,
                                                          on_source_window.end,
                                                          seg.start, seg.end))
        sys.exit(1)

# Here we create the on source segment
on_source_window_segment = SegmentList([Segment(on_source_start_time, on_source_start_time + onSourceWindowLength)])
if args.search_type == 'grb':
    on_source_window_segment.write('input/on_source_interval.txt')
elif args.search_type in ['sn', 'allysky']:
    on_source_window.write('input/on_source_interval.txt')

if args.search_type == 'grb':
    # Make a set of segments for the off_source jobs
    offsource_full_segment_list = copy.deepcopy(full_segment_list)
    offsource_full_segment_list = [iseg - on_source_window_segment for iseg in offsource_full_segment_list]
    for iseglist in offsource_full_segment_list:
        for iseg in range(len(iseglist)):
            iseglist[iseg] = Segment(iseglist[iseg])


    coincidence_off_source = reduce(operator.and_, offsource_full_segment_list)
    coincidence_on_source = on_source_window_segment or reduce(operator.and_, full_segment_list)

# ---- Split coincidence segment list into "chunks".
    coincidence_on_source_segment_chunks = setuputils.make_chunks(coincidence_on_source, blockTime, 2*transientTime)
elif args.search_type in ['sn', 'allsky']:
    # If SN then on source and off source are the same soo....
    offsource_full_segment_list = copy.deepcopy(full_segment_list)
    coincidence_on_source = reduce(operator.and_, full_segment_list)
    coincidence_off_source = reduce(operator.and_, full_segment_list)
    coincidence_on_source_segment_chunks = setuputils.make_chunks(coincidence_on_source, blockTime, 2*transientTime)
    # Make use of unused data
    if abs(coincidence_on_source - coincidence_on_source_segment_chunks):
        coincidence_on_source_segment_chunks.append(Segment(Segment(coincidence_on_source.extent()).end - blockTime, Segment(coincidence_on_source.extent()).end))

coincidence_on_source_segment_chunks.write('input/segment_on_source.txt')


# Write on_source_window to file
fwin = open('input/window_on_source.txt', 'w')
fwin.write(str(int(trigger_time)) + ' 0' * len(detector) +'\n')
fwin.close()

if args.search_type == 'grb':
    # For GRB search make sure coincident on-source fully overlaps  with on-source window(i.e. there are no holes with it and the newly made coincident on-source segment.
    if not on_source_window in coincidence_on_source:
        logging.error("Error: on-source period is not a coincidence segment of the specified network.")
        sys.exit(1)

    # We must choose a sky grid and then loop over the on source segments with this selected grid and shift the grid slightly for each on source chunk as the earth has rotated
    # slightly for each chunk. We then proceed to do the same things with the injects and off source
    # ---- Generate sky positions for patch about ra, dec at trigger_time.
    if int(cp.get('input','usexchooseskylocations')) and sky_pos_err:
        skyPosFilename_trigger = 'input/sky_positions_trigger_time.txt'
        if grid_type == 'healpix' or grid_type == 'circular':
            logger.info("Calling xmakeskygrid to generate {0}".format(skyPosFilename_trigger))
            ra_str = str(ra)
            decl_str = str(decl)
            sky_pos_err_str = str(sky_pos_err)
            xmakeskygridCall = ' '.join([ "xmakeskygrid", ra_str, decl_str,
                str(trigger_time), sky_pos_err_str, cp.get('input','numSigmaSkyPos'),
                siteStrTilde, cp.get('input','delayTol'), skyPosFilename_trigger,
                grid_type, '0'])
            logger.info(xmakeskygridCall)
            os.system(xmakeskygridCall)

        elif grid_type == 'file':
            logger.info("Calling xconvertfilegrid to generate {0}".format(skyPosFilename_trigger))
            xconvertfilegridCall = ' '.join(['xconvertfilegrid',str(sky_pos_err),str(trigger_time),skyPosFilename_trigger])
            logger.info(xconvertfilegridCall)
            os.system(xconvertfilegridCall)

        else:
            logger.error("Choice of sky position grid is unrecognized. Please use circular or line.")


f = open('input/event_on_source.txt', 'w')
fseg = open('input/segment_on_source.txt', 'w')
# For each on source segment choose the sky locations
for iSeg in range(len(coincidence_on_source_segment_chunks)):
    # ---- Write event to event file.
    time_range_string = str((float(coincidence_on_source_segment_chunks[iSeg].start) + \
                            float(coincidence_on_source_segment_chunks[iSeg].end)) / 2) + \
                            ' 0' * len(detector)  + '\n'
    f.write(time_range_string)
    # ---- Write segment to segment file.
    time_range_string = '0 ' + str(int(coincidence_on_source_segment_chunks[iSeg].start)) \
                        + ' ' + str(int(coincidence_on_source_segment_chunks[iSeg].end)) \
                        + ' ' + str(int(coincidence_on_source_segment_chunks[iSeg].end \
                        - coincidence_on_source_segment_chunks[iSeg].start)) + '\n'
    fseg.write(time_range_string)


    gpsBlockCenterTime = (float(coincidence_on_source_segment_chunks[iSeg].start) + float(coincidence_on_source_segment_chunks[iSeg].end))/2

    if args.search_type == 'grb':

        # ---- If user has set usexchooseskylocations flag to 1 in params file we now
        #      compute a list of sky positions we need to search and write these to a
        #      file.

        if int(cp.get('input','usexchooseskylocations')) and sky_pos_err:
            skyPosFilename = 'input/sky_positions_' + str(iSeg) + '.txt'
            logger.info("Calling xconvertskylocations to generate {0}".format(skyPosFilename))
            xconvertskylocationsCall = ' '.join([ "xconvertskylocations",
                str(skyPosFilename_trigger),
                str(trigger_time),
                str(gpsBlockCenterTime),
                skyPosFilename])
            logger.info(xconvertskylocationsCall)
            os.system(xconvertskylocationsCall)

            # ---- Check that the output file has been created.
            if not os.path.isfile(skyPosFilename):
                logger.error("Error creating {0}".format(skyPosFilename))

            # ---- Count number of sky positions.
            numSky = len(open(skyPosFilename).readlines())

            # ---- Set skyPositionList arg that will be written to matlab format
            #      parameters files.
            skyPositionList = skyPosFilename

        # ---- If user has not specified a sky position error we will search only a
        #      single sky position.
        else:
            # ---- Convert right ascension, declination to Earth-fixed coordinates.
            skyutils.radectoearth(ra, decl, gpsBlockCenterTime, filename='input/earthfixedcoordinates' + str(iSeg) + '.txt')
            phi, theta = skyutils.radectoearth(ra, decl, gpsBlockCenterTime)
            # ---- Load earth-fixed coordinates back in.
            logger.info("GRB position in Earth-fixed coordinates (rad):")
            logger.info("    phi = {0}".format(phi))
            logger.info("    theta = {0}".format(theta))

            # ---- Set number of sky positions.
            numSky = 1

            # ---- Set skyPositionList arg that will be written to matlab format
            #      parameters files.
            skyPositionList = '[' + str(theta) + ',' + str(phi) + ']'

    elif args.search_type in ['sn', 'allsky']:

        # ---- Convert right ascension, declination to Earth-fixed coordinates.
        skyutils.radectoearth(ra, decl, gpsBlockCenterTime, filename='input/earthfixedcoordinates' + str(iSeg) + '.txt')
        phi, theta = skyutils.radectoearth(ra, decl, gpsBlockCenterTime)
        # ---- Load earth-fixed coordinates back in.
        logger.info("SN position in Earth-fixed coordinates (rad):")
        logger.info("    phi = {0}".format(phi))
        logger.info("    theta = {0}".format(theta))

        # ---- Set number of sky positions.
        numSky = 1

        # ---- Set skyPositionList arg that will be written to matlab format
        #      parameters files.
        skyPositionList = '[' + str(theta) + ',' + str(phi) + ']'

    # -----------------------------------------------------------------------------
    #      Write Matlab-formatted parameters files.
    # -----------------------------------------------------------------------------

    # ---- We will write three sets of parameters files:  one on-source file,
    #      one off-source file, and (for each waveform set and injection scale)
    #      one injections file.

    # ---- Status message.
    logger.info("Writing Matlab-formatted parameter files ...")

    # ---- Parameters file for on-source analysis.
    ioutils.write_onsource('input/parameters_on_source_' + str(iSeg) + '.txt', outputType, frameCacheAll, skyPositionList, likelihoodTypeStr, onSourceEndOffset, cp)

    # ---- Parameters file for ul-source analysis.
    ioutils.write_ul_source('input/parameters_ul_source_' + str(iSeg) + '.txt',outputType, frameCacheAll, skyPositionList, likelihoodTypeStr, onSourceEndOffset, cp)

    # ---- Parameters file for off-source analysis.
    ioutils.write_offsource('input/parameters_off_source_' + str(iSeg) + '.txt', outputType, frameCacheAll, skyPositionList, likelihoodTypeStr, onSourceEndOffset, cp)

    # ---- Parameter files for on-the-fly simulated waveform analyses, if requested.
    if cp.has_section('waveforms') :
        # ---- Read [injection] parameters.  If waveform_set is empty then no
        #      files will be written.
        waveform_set = cp.options('waveforms')
        # ---- Write one parameters file for each (waveform set, injection scale) pair.
        for iwaveform in waveform_set :
            if cp.has_section(iwaveform) & cp.has_option(iwaveform,'injectionScales') :
                # ---- This check lets you specify different injection scales and
                #      spacing for each waveform set.
                injectionScalesList = cp.get(iwaveform,'injectionScales')
                injectionScales = injectionScalesList.split(',')
            else:
                # ---- Otherwise, use the injection scales and spacing specified in
                #      the [injection] section.
                injectionScalesList = cp.get('injection','injectionScales')
                injectionScales = injectionScalesList.split(',')

            # ---- If we do long injections we will have one single parameter file
            ioutils.write_waveforms("input/parameters_simulation_" + iwaveform + "_0_" + str(iSeg) + ".txt", outputType, frameCacheAll, skyPositionList, likelihoodTypeStr, onSourceEndOffset, cp, injectionScalesList, disableFastInjections, iwaveform)


f.close()
fseg.close()

# ---- Append to summary file.
sfile = open(summary_file,'a')
sfile.write('\t' + str(numSky))
sfile.close()

# ---- Status message.
logger.info("... finished writing on-source event file.")


# -------------------------------------------------------------------------
#      Determine MDCs to process, write mdcChannelFiles.
# -------------------------------------------------------------------------

# ---- Parameter files for MDC waveform analyses, if requested.
if cp.has_option('mdc','mdc_sets') :
    # ---- Read [mdc] sets and injection scales.  If mdc_sets is empty or specifies
    #      unknown MDC sets then the script will have already exited when trying to
    #      write the mdcchannel file above.
    ioutils.write_mdc(filename, outputType, frameCacheAll, skyPositionList, likelihoodTypeStr, parameters, onSourceEndOffset, cp, injectionScalesList, disableFastInjections, waveform)

# ---- Status message.
logger.info("... finished writing parameter files.     ")


# -------------------------------------------------------------------------
#    Copy waveform catalogs (if specified) to input/.
# -------------------------------------------------------------------------

if catalog_dir:
    logger.info("Copying waveform catalogs to input/ ...")
    cpCommand = ' '.join(['cp ' + catalog_dir + '/*.mat input/'])
    os.system(cpCommand)
    logger.info("... finished copying waveform catalogs.")


# -------------------------------------------------------------------------
#    Make off-source coincidence segment lists for all lags.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("Writing off-source event file ...          ")

# ---- Keep track of number of off-source events.
numOff = 0

if args.search_type == 'grb':
    #If this is a grb search we can use some of the background in zero lag to build stats. Otherwise if
    # it is sn or allsky then all of the zero lag "background" or "offsource" is used for the on_source trials.
    #Therefore, we can only use lags to generate off source trials for those.

    # ---- Split coincidence segment list into "chunks".
    coincidence_off_source_segment_chunks = setuputils.make_chunks(coincidence_off_source, onSourceWindowLength, 2*transientTime)

    # ---- Make event list and segment list files for the zero-lag off-source times.
    f = open('input/window_off_source.txt', 'w')

    # ---- List of lags for each detector (0 in this case)
    line = ' 0' * len(detector)

    for iSeg in range(len(coincidence_off_source_segment_chunks)):
        # ---- Write event to event file.
        time_range_string = str(int(coincidence_off_source_segment_chunks[iSeg].start - \
                                  onSourceBeginOffset + transientTime)) + line + '\n'
        f.write(time_range_string)
    f.close()

# ---- Now open lag file (if any) and write event file for all non-zero lags.
# ---- We'll supply network dependent-defaults.
#      Lag file will have one lag per detector; any of them may be zero.
if lagFile:
    if args.search_type == 'grb':
        f = open('input/window_off_source.txt', 'a')
    elif args.search_type in ['sn', 'allsky']:
        f = open('input/event_off_source.txt', 'w')
        fseg = open('input/segment_off_source.txt', 'w')

    lag_list = open(lagFile, mode='r')
    for line in lag_list:
        # ---- Extract time lag for each detector.
        lags = line.split(None)
        if len(lags) != len(detector):
            logger.error("the lag file should have number of "
            "columns equal to the the number of detectors we are analysing. "
            " Lag file: 0{}".format(lagFile))

        # ---- Make a time-lagged copy of the segment lists.
        lag_full_segment_list = copy.deepcopy(offsource_full_segment_list)
        # ---- Time shift segment list of each detector.
        for idet in range(len(detector)):
            for isegments in range(len(lag_full_segment_list[idet])):
                lag_full_segment_list[idet][isegments] = lag_full_segment_list[idet][isegments].shift(-int(lags[idet]))

        coincidence_off_source_lag = reduce(operator.and_, lag_full_segment_list)

        if args.search_type == 'grb':
            lag_coincidence_off_source_segment_chunks = setuputils.make_chunks(coincidence_off_source_lag, onSourceWindowLength, 2*transientTime)
            for iSeg in range(len(lag_coincidence_off_source_segment_chunks)):
                # ---- Write event to event file.
                time_range_string = str(int(lag_coincidence_off_source_segment_chunks[iSeg].start - \
                                          onSourceBeginOffset + transientTime)) +  ' ' + line
                f.write(time_range_string)

        elif args.search_type in ['sn', 'allsky']:
            # ---- Split coincidence segment list into "chunks".
            lag_coincidence_off_source_segment_chunks = setuputils.make_chunks(coincidence_off_source_lag, blockTime, 2*transientTime)
            # If this is a SN search then we want to make an additonal segment using any unused segment data.
            if abs(coincidence_off_source_lag - lag_coincidence_off_source_segment_chunks):
                lag_coincidence_off_source_segment_chunks.append(Segment(Segment(coincidence_off_source_lag.extent()).end - blockTime, Segment(coincidence_off_source_lag.extent()).end))

            for iSeg in range(len(lag_coincidence_off_source_segment_chunks)):
                # ---- Write event to event file.
                time_range_string = str((int(lag_coincidence_off_source_segment_chunks[iSeg].start) \
                    + int(lag_coincidence_off_source_segment_chunks[iSeg].end)) / 2)
                time_range_string = time_range_string + ' ' + line
                f.write(time_range_string)
                # ---- Write segment to segment file.
                time_range_string = '0 ' + str(int(lag_coincidence_off_source_segment_chunks[iSeg].start)) \
                    + ' ' + str(int(lag_coincidence_off_source_segment_chunks[iSeg].end)) \
                    + ' ' + str(int(lag_coincidence_off_source_segment_chunks[iSeg].end) \
                    - int(lag_coincidence_off_source_segment_chunks[iSeg].start)) + '\n'
                fseg.write(time_range_string)
                numOff = numOff + 1

    if args.search_type == 'grb':
        f.close()
    elif args.search_type in ['sn', 'allsky']:
        f.close()
        fseg.close()

# ---- Status message.
logger.info("... finished writing off-source event file.")

# -----------------------------------------------------------------------------
#      Read in off-source event file and check each event passes meets our
#                            network criteria.
# -----------------------------------------------------------------------------
if args.search_type == 'grb':
# KLUDGE so that we don't have to reindent the code below

    logger.info("Applying network criteria to our off-source events.")

    # ---- Get triggerTimes and timeOffsets for each off-source event.

    # ---- Write file containing off-source event centre times,
    #      trigger_time_off.txt using input/event_off_source.txt.
    awkCommand = ' '.join(["awk",
            "'{print $1}'",
            "input/window_off_source.txt",
            ">", "input/trigger_time_off.txt"])
    os.system(awkCommand)

    # ---- Write file containing off-source event lag times,
    #      time_offsets_off.txt using input/event_off_source.txt.

    awkStr = ''
    for idx in range(0,len(detector)):
        awkStr = awkStr +  " \" \" " + " $" + str(idx+2)

    awkCommand = ' '.join(["awk",
        "'{print ", awkStr, "}'",
        "input/window_off_source.txt",
        ">", "input/time_offsets_off.txt"])
    os.system(awkCommand)

    # ---- Read in event_off_source.txt
    fevents = open('input/window_off_source.txt','r')
    events = fevents.readlines()
    fevents.close()

    # ---- Find network for each off-source event using xnetworkselection.m.
    if not cp.has_option('parameters','makeSimulatedNoise'):
        network_outputFile = 'input/xnetworkselection_offsource.dat'

        xnetworkselectionCall = ' '.join([ "xnetworkselection",
            detectorStr,
            "input/trigger_time_off.txt",
            "input/time_offsets_off.txt",
            cat1_segment_file_list['H1'],
            "None",
            cat1_segment_file_list['L1'],
            cat1_segment_file_list['G1'],
            cat1_segment_file_list['V1'],
            cat24_segment_file_list['H1'],
            "None",
            cat24_segment_file_list['L1'],
            cat24_segment_file_list['G1'],
            cat24_segment_file_list['V1'],
            network_outputFile,
            str(onSourceBeginOffset),
            str(onSourceEndOffset),
            str(transientTime),
            str(onSourceWindowLength)
            ])

        logger.info(xnetworkselectionCall)
        os.system(xnetworkselectionCall)

        # ---- Read in network strings from network_outputFile.
        fnetworks = open(network_outputFile,'r')
        networks = fnetworks.readlines()
        fnetworks.close()
    else:
        networks = [''.join(detector)] * len(events)

    # ---- Discard off-source events which do not contain the ifos in
    #      our network.

    eventListTempFilename = "input/window_off_source_temp.txt"
    eventListTemp = open(eventListTempFilename,'w')

    # ---- Check events and networks list have the same length.
    if not(len(events) == len(networks)):
        logger.error("off-source networks and events lists "
            "should have the same length")


    # ---- Number of off-source events before we discard any.
    # ---- This is equivalent to the number of zero and external lags
    # ---- mulitplied by the number of internal lags
    # See xdetection.m line 1045
    internalSlides = 1
    if cp.has_option('parameters','circtimeslidestep'):
        circTimeSlides = cp.getint('parameters','circtimeslidestep')
        if len(detector) == 1:
            internalSlides = 1
        elif len(detector) == 2:
            internalSlides = np.arange(0,blockTime-2*transientTime-circTimeSlides,circTimeSlides).size
        elif len(detector) == 3:
            internalSlides = np.arange(0,blockTime/2-transientTime-circTimeSlides,circTimeSlides).size
        else:
            logger.error('Circtimeslides not set up for number of detectors given')

    origNumOff = len(events) * internalSlides

    # ---- Keep track of how many off-source jobs we have after
    #     discarding those with bad networks.
    numOff = 0

    # ---- If the user has specified the number of background jobs
    #      get this variable.
    if cp.has_option('background','numJobs'):
        background_numJobs = int(cp.get('background','numJobs'))
        if background_numJobs > origNumOff:
            logger.error( "We have only {0} background jobs".format(origNumOff))
            logger.error( "Specify more lags to get desired "
                "number of background jobs: numJobs = {0}".format(background_numJobs))

    elif FAP or FAR:
        if FAP > 1.0:
            background_numJobs = 2*(100./(100.0-FAP))
        elif (FAP > 0) and (FAP < 1):
            background_numJobs = 2 * (100./FAP)
        else:
            logger.error('FAP number unusual, are you sure this is what you want')
        if background_numJobs > origNumOff:
            logger.error( "We have only {0} background jobs".format(origNumOff))
            logger.error( "Specify more lags to get desired "
                "number of background jobs: numJobs = {0}".format(background_numJobs))
    else:
        background_numJobs = origNumOff

    # ---- Do these strings contain the ifos in the detector list.
    for idx in range(0,len(networks)):
        # ---- For each off-source network we will count how many
        #      ifos from our selected network it contains.
        # ---- We will discard any off-source events whose corresponding
        #      network does not contain all of the ifos in our
        #      selected network.
        numIfo = 0

        # ---- skip network if it is not well determined
        if networks[idx][0] != 'X':
            # ---- Loop over ifos in our selected network.
            for ifo in detector:
                numIfo = numIfo + networks[idx].count(ifo)

        # ---- If the off-source network contains all of the ifos
        #      in our selected network we will write out the details
        #      of the corresponding off-source event to a temporary file.
        if numIfo == len(detector):
            if numOff < background_numJobs:
                eventListTemp.write(events[idx])
                numOff += 1*internalSlides

    eventListTemp.close()

    # ---- Replace events_off_source.txt with new updated file containing
    #      only those events whose networks contain the ifos in our
    #      selected network.
    mvCommand = ' '.join(['mv',
        eventListTempFilename,
        'input/window_off_source.txt'])
    os.system(mvCommand)

    logger.info("We retain the " + str(numOff) +  " jobs from a total of "
        + str(origNumOff) + " off-source jobs that satisfy " + "our network criteria.")

    # ---- If we do not have the required number of off-source jobs tell
    #      the user and exit.
    if cp.has_option('background','numJobs'):
        background_numJobs = int(cp.get('background','numJobs'))
        if numOff < background_numJobs:
            logger.error("Specify more lags to get desired "\
                "number of background jobs: numJobs = {0}".format( background_numJobs))

    # ---- Split window list into event list
    fwin = open('input/window_off_source.txt','r')
    windowList = fwin.readlines()
    fwin.close()

    fev = open('input/event_off_source.txt','w')
    for window in windowList:
        windowWord=window.split()
        for iJob in range(jobsPerWindow):
            time_range_string = str(float(windowWord[0])+onSourceBeginOffset+(blockTime-2*transientTime)*(0.5+iJob))
            for iWord in range(1,len(windowWord)):
                time_range_string = time_range_string + ' ' + windowWord[iWord]
            time_range_string = time_range_string + '\n'
            fev.write(time_range_string)
    fev.close()

elif args.search_type in ['sn', 'allsky']:
    event_on_file = open('input/event_on_source.txt')
    event_on_list = event_on_file.readlines()
    nOnEvents = len(event_on_list)
    event_on_file.close()
    # count the number of effective bkg trials
    nEffBkgTrials = int(math.floor(float(numOff)/float(nOnEvents)))
    fwin = open('input/window_off_source.txt','w');
    for iTrial in range(0,nEffBkgTrials):
      fwin.write(str(int(trigger_time)) + "\n")
    fwin.close()
    # throw away events that would form a last non complete effective bkg trial
    # ---- Read the number of chunks in on-source from the event list
    event_off_file = open('input/event_off_source.txt')
    event_off_list = event_off_file.readlines()
    event_off_file.close()
    event_off_file = open('input/event_off_source.txt','w')
    for iEvent in range(0,nEffBkgTrials*nOnEvents):
      event_off_file.write(event_off_list[iEvent])
    event_off_file.close()
    logger.info("We retain the " + str(numOff) +  " jobs from a total of "
        + str(numOff) + " off-source jobs that satisfy " + "our network criteria.")

# ---- Append to summary file.
sfile = open(summary_file,'a')
sfile.write('\t' + str(numOff))
sfile.close()


# -------------------------------------------------------------------------
#      Write injection files for on-the-fly simulations, if needed.
# -------------------------------------------------------------------------

# ---- construct time segment(s) into which to injecting, choice between on-source and off-source injections
if offSourceInj:
    # ---- Off-source injections
    os.system('cp input/window_off_source.txt input/window_inj_source.txt')
    os.system('cp input/event_off_source.txt input/event_inj_source.txt')
else:
    # ---- On-source injections
    os.system('cp input/window_on_source.txt input/window_inj_source.txt')
    os.system('cp input/event_on_source.txt input/event_inj_source.txt')
injTimeRangeStr = ""
injTimeOffsetStr = ""
injCenterTime = []
injwindows = loadtxt('input/window_inj_source.txt')
event_inj_list = loadtxt('input/event_inj_source.txt')
if injwindows.ndim == 1:
    injwindows = [injwindows]
if event_inj_list.ndim == 1:
    event_inj_list = [event_inj_list]
for iEv in range(0,len(injwindows)):
    # ---- skip time-slid off-source, assume they are only at the top of
    #      the window list, this assumption is used later in the code
    if sum(abs(injwindows[iEv][1:])) > 0:
        break
    injTimeRangeStr = injTimeRangeStr + str(injwindows[iEv][0] + onSourceBeginOffset) + \
        '~' + str(injwindows[iEv][0] + onSourceEndOffset) + '~'
    injTimeOffsetStr = injTimeOffsetStr + str(int(trigger_time)-injwindows[iEv][0]) + '~'
    injCenterTime.append(injwindows[iEv][0])
# remove trailing tilde
injTimeRangeStr = injTimeRangeStr[:-1]
injTimeOffsetStr = injTimeOffsetStr[:-1]

if cp.has_section('waveforms') :
    # ---- Read [injection] parameters.
    waveform_set = cp.options('waveforms')
    injectionInterval = cp.get('injection','injectionInterval')
    # ---- If we have second error circle, then adjust injectionInterval by a
    #      factor of 2 to put half the injections in each circle.

    logger.info("Making injection files ...")
    for set in waveform_set :
        waveforms = cp.get('waveforms',set)
        baseinjfilename = "injection_" + set + ".txt"
        injfilename = "input/injection_" + set + ".txt"

        # ---- Use premade injection files if they exist [SARAH]
        if waveforms == baseinjfilename :
            command = ' '.join(["cp ", baseinjfilename, " input/"])
            os.system(command)
            logger.info("    Using-premade injection file: {0}".format(injfilename))

  	# ---- setUpJobs script that was here before [SARAH]
        else:
            # ---- Construct command to make injection file for this waveform set.
            # ---- Wrap waveforms string in single quotes to escape any ; characters,
            #      which are used for generating random injection parameters.
            tmpwaveforms = ''.join(["'", waveforms, "'"])
            make_injection_file_command = ' '.join(["xmakegrbinjectionfile",
            injfilename, tmpwaveforms,
            injTimeRangeStr,
            injTimeOffsetStr,
            injectionInterval, str(ra), str(decl), str(seed), grid_type, grid_sim_file ])
            # ---- Issue command to make injection file for this waveform set.
            # ---- We'll overwrite this file later with miscalibrated and/or
            #      jittered versions, if desired.
            logger.info("    Writing : {0}".format(injfilename))
            logger.info("{0}".format(make_injection_file_command))
            os.system(make_injection_file_command)
            if injdistrib:
                jitter_injection_command = ' '.join(["xjitterinjectionskypositions",
                injfilename,
                injfilename,
                injdistrib ])
                logger.info("    Jittering: {0}".format(injfilename))
                logger.info("{0}".format(jitter_injection_command))
                os.system(jitter_injection_command)

        # ---- Apply calibiration uncertainties to injections if required. We use
        #      the real detector (rather than the virtual detector) for determining
        #      calibration uncertainties, as the virtual detector may not have
        #      been operational at the time of the trigger.
        if int(cp.get('injection','miscalibrateInjections'))==1:
            miscalib_injection_command = ' '.join(["xmiscalibrategrbinjectionfile",
            injfilename,
            injfilename,
            detectorStrTilde,
            frameTypeStrTilde ])
            logger.info("    Miscalibrating :{0}".format(injfilename))
            logger.info("{0}".format(miscalib_injection_command))
            os.system(miscalib_injection_command)

    logger.info("... finished making injection files.")


# -------------------------------------------------------------------------
#      Create ini file for xtmva.py, if required.
# -------------------------------------------------------------------------

# ---- Procedure:
#      -> Copy the xtmva codes from the tmvadir location specified in the grb
#         ini file to a new local directory called xtmvapy. This local copy
#         will be used for the analysis to avoid conflicts if other xtmva
#         analyses are running simultaneously.
#      -> Write a bespoke ini file. Place it in the local run directory since
#         xtmva.py will look for it there.
#      -> To do: add a "post" script to the all_jobs dag that runs xtmva.py.
#         In fact this is probably easier to do in xgrbwebpage.py.
if xtmvaFlag:
    # ---- Extract required data from [xtmva] section.
    if cp.has_section('xtmva'):
        WFtrain    = cp.get('xtmva','WFtrain')
        classifier = cp.get('xtmva','classifier')
        tmvadir    = cp.get('xtmva','tmvadir')
        # ---- Copy xtmva scripts to the local directory.
        logger.info("Creating ./xtmvapy directory for xtmva analysis.")
        os.system('rm -rf xtmvapy')
        os.system('mkdir xtmvapy')
        os.system('cp ' + tmvadir + '/*.* xtmvapy')
        # ---- Write ini file.
        logger.info("Generating ini file for xtmva analysis.")
        xtmvainifile = open('./xtmva.ini','w')
        inistring = '[Settings]' + '\n'
        xtmvainifile.write(inistring)
        inistring = 'WFtrain=' + WFtrain + '\n'
        xtmvainifile.write(inistring)
        inistring = 'classifier=' + classifier + '\n'
        xtmvainifile.write(inistring)
        inistring = '[Folders]' + '\n'
        xtmvainifile.write(inistring)
        inistring = 'tmvadir=' + os.getcwd() + '/xtmvapy' + '\n'
        xtmvainifile.write(inistring)
        inistring = 'xout=xtmva_' + classifier + '\n'
        xtmvainifile.write(inistring)
        inistring = '[WFtest]' + '\n'
        xtmvainifile.write(inistring)
        if cp.has_section('waveforms') :
            waveform_set = cp.options('waveforms')
            for set in waveform_set :
                # ---- Add all sets to the testing list EXCEPT for the traininiget.
                if not (set==WFtrain):
                    # ---- Read the number of injections from the log file.
                    injfilename = "input/injection_" + set + ".txt"
                    injfile = open(injfilename)
                    injlist = injfile.readlines()
                    ninj = len(injlist)
                    injfile.close()
                    inistring = set + '=' + str(ninj) + '\n'
                    xtmvainifile.write(inistring)
        inistring = '[parameters]' + '\n'
        xtmvainifile.write(inistring)
        inistring = 'likelihoods='+likelihoodTypeStr + '\n'
        xtmvainifile.write(inistring)
        xtmvainifile.close()
    else:
        logger.error("Error: xtmva analysis requested by ini file does not have an [xtmva] section. Exiting.")

# -------------------------------------------------------------------------
#    Preparations for writing dags.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("Writing job submission files ... ")

# ---- Retrieve job retry number from parameters file.
if cp.has_option('condor','retryNumber'):
    retryNumber = int(cp.get('condor','retryNumber'))
else:
    retryNumber = 0
if smartCluster == True:
  retryNumberClustered = 0

# ---- DAGman log file.
#      The path to the log file for condor log messages. DAGman reads this
#      file to find the state of the condor jobs that it is watching. It
#      must be on a local file system (not in your home directory) as file
#      locking does not work on a network file system.
log_file_on_source = cp.get('condor','dagman_log_on_source')
log_file_off_source = cp.get('condor','dagman_log_off_source')
if smartCluster == True:
    log_file_on_source_clustered = cp.get('condor','dagman_log_on_source')+'_clustered'
    log_file_off_source_clustered = cp.get('condor','dagman_log_off_source')+'_clustered'
if cp.has_section('waveforms') :
    log_file_simulations = cp.get('condor','dagman_log_simulations')
    if smartCluster == True:
        log_file_simulations_clustered = cp.get('condor','dagman_log_simulations')+'_clustered'
if cp.has_option('mdc','mdc_sets') :
    log_file_mdcs = cp.get('condor','dagman_log_mdcs')

# ---- Make directories to store the log files and error messages
#      from the nodes in the DAG
try: os.mkdir( 'logs' )
except: pass
# ---- Make directory to store the output of our test job.
#      NOT TO BE DONE FOR PRODUCTION RUNS!
try: os.mkdir( 'output' )
except: pass
try: os.mkdir( 'output/on_source' )
except: pass
try: os.mkdir( 'output/ul_source' )
except: pass
try: os.mkdir( 'output/off_source' )
except: pass
# ---- now make the same directories only for clustered results.
if smartCluster == True:
  try: os.mkdir( 'output_clustered' )
  except: pass
  try: os.mkdir( 'output_clustered/on_source' )
  except: pass
  try: os.mkdir( 'output_clustered/off_source' )
  except: pass
# ---- Make directories to hold results of injection runs - one for each
#      waveform and injection scale.
if cp.has_section('waveforms') :
    waveform_set = cp.options('waveforms')
    for set in waveform_set :
        # ---- The user can specify different injection scales for each
        #      waveform set by including a section with that name in the ini
        #      file.  Look for one.  If no special section for this waveform,
        #      then use the default injection scales from [injection].
        if cp.has_section(set) & cp.has_option(set,'injectionScales') :
            injectionScalesList = cp.get(set,'injectionScales')
            injectionScales = injectionScalesList.split(',')
        else:
            injectionScalesList = cp.get('injection','injectionScales')
            injectionScales = injectionScalesList.split(',')
        scale_counter = 0
        for injectionScale in injectionScales :
            try: os.mkdir( 'output/simulations_' + set + '_' + str(scale_counter) )
            except: pass
            scale_counter = scale_counter + 1
            if smartCluster == True:
              try: os.mkdir( 'output_clustered/simulations_' + set + '_' + str(scale_counter) )
              except: pass
if cp.has_option('mdc','mdc_sets') :
    mdc_setsList = cp.get('mdc','mdc_sets')
    mdc_sets = mdc_setsList.split(',')
    for set in mdc_sets :
        # ---- The user can specify different injection scales for each
        #      waveform set by including a section with that name in the ini
        #      file.  Look for one.  If no special section for this waveform,
        #      then use the default injection scales from [injection].
        if cp.has_section(set) & cp.has_option(set,'injectionScales') :
            injectionScalesList = cp.get(set,'injectionScales')
            injectionScales = injectionScalesList.split(',')
        else:
            injectionScalesList = cp.get('injection','injectionScales')
            injectionScales = injectionScalesList.split(',')
        scale_counter = 0
        for injectionScale in injectionScales :
            try: os.mkdir( 'output/simulations_' + set + '_' + str(scale_counter) )
            except: pass
            scale_counter = scale_counter + 1


# -------------------------------------------------------------------------
#      Prepare xtmva dag.
# -------------------------------------------------------------------------

# ---- We initalise the dag here but do not write it until after all other
#      dags have been written (on-source, off-source, simulations). This is
#      because the xtmva dag is unique in that all other dags must complete
#      before it can run. We fix this dependency by adding all the xmerge
#      jobs as parents to the xtmva job.
if xtmvaFlag:
    logger.info("Writing xtmva dag ... ")
    log_file_xtmva = cp.get('condor','dagman_log_on_source')+'_xtmva'
    logger.info(log_file_xtmva)
    dag_xtmva = pipeline.CondorDAG(log_file_xtmva + uuidtag)
    dag_xtmva.set_dag_file( 'grb_xtmva' )
    xtmvajob  = xcondor.XtmvaJob(cp,len(detector))
    xtmvanode = xcondor.XtmvaNode(xtmvajob)
    xtmvanode.set_retry("1")
    xtmvanode.set_name("xtmva_on_source_" + xtmvanode.get_name())

# -------------------------------------------------------------------------
#      Write on-source dag.
# -------------------------------------------------------------------------

# ---- Create a dag to which we can add jobs.
dag = pipeline.CondorDAG(log_file_on_source + uuidtag)

# ---- Set the name of the file that will contain the DAG.
dag.set_dag_file( 'grb_on_source' )

# ---- Make instance of XsearchJob.
job = xcondor.XsearchJob(cp)

if outputType == 'seedless':
    if doGPUOnSource == "true":
        job = xcondor.XsearchGPUJob(cp)

# ---- Make instance of XmergeJob.
mergejob = xcondor.XmergeJob(cp)
# ---- Put a single merge job node in mergejob.  This job will combine
#      into a single file the output of all of the off-source analysis jobs.
mergenode = xcondor.XmergeNode(mergejob)
if xtmvaFlag:
    xtmvanode.add_parent(mergenode)

# ---- if clustering.
if smartCluster == True:
  logger.info("Writing on-source clustering dag ... ")
  logger.info(log_file_on_source_clustered)
  dag_clustered = pipeline.CondorDAG(log_file_on_source_clustered + uuidtag)
  dag_clustered.set_dag_file( 'grb_on_source_clustered' )
  mergeclusteredjob = xcondor.XmergeClusteredJob(cp,len(detector))
  mergeclusterednode = xcondor.XmergeClusteredNode(mergeclusteredjob)

# ---- Make analysis jobs for all segments (currently 1) in the on-source
#      segment list.
segmentList = pipeline.ScienceData()
segmentList.read( 'input/segment_on_source.txt' , blockTime )

# ---- Read the number of chunks from the event list
event_on_file = open('input/event_on_source.txt')
event_on_list = event_on_file.readlines()
nOnEvents = len(event_on_list)
event_on_file.close()

# ---- Read from the parameters file how job results files are to be
#      distributed across nodes, and write that info to a file that
#      will be used by the post-processing scripts.
distributeOnSource = int(cp.get('output','distributeOnSource'))
if distributeOnSource == 1:
    jobNodeFileOnSource = cp.get('output','jobNodeFileOnSource')
    nodeList = open( jobNodeFileOnSource,'w')
    nodeList.write("Number_of_detectors %d\n"%(len(detector)))
    for i in range(0,len(detector)) :
        nodeList.write("%s\t"%(detector[i]))
    nodeList.write("\n")
    nodeList.write('event_on_source_file ')
    nodeList.write(cwdstr + "/input/event_on_source.txt \n")
    nodeList.write("Injection_file N/A \n")
    nodeList.write('Number_of_jobs ')
    nodeList.write("%d \n"%(len(segmentList)))
    nodeList.write('Number_of_jobs_per_node ')
    nodeList.write("%d \n"%(len(segmentList)))
    nodeList.write(cwdstr + " \n")

# ---- Add one node to the job for each segment to be analysed.
for i in range(len(segmentList)):
    node = xcondor.XsearchNode(job)
    # ---- Parameters file:
    matlab_param_file = cwdstr + "/input/parameters_on_source_" + str(i) + ".txt"
    node.set_param_file(matlab_param_file)
    node.set_x_jobnum(i)
    if distributeOnSource == 1 :
        nodeList.write(cwdstr + "/output/on_source" + "/results_%d.mat \n"%(i))
    # ---- On-source results files are always written to the local
    #      output/on_source directory - there are very few such jobs.
    node.set_output_dir( os.path.join( cwdstr + "/output/on_source" ) )
    node.set_x_injnum('0')
    mergenode.add_parent(node)
    node.set_retry(retryNumber)
    # ---- Prepend human readable description to node name.
    node.set_name("xdetection_on_source_seg" + str(i) + "_" + node.get_name())
    dag.add_node(node)

# ---- Supply remaining XmergeJob node parameters, add job to the dag.
mergenode.set_dir_prefix("on_source/")
mergenode.set_sn_flag("0 " + mergingCutsString)
mergenode.set_retry(retryNumber)
# ---- Prepend human readable description to node name.
mergenode.set_name("xmerge_on_source_" + mergenode.get_name())
dag.add_node(mergenode)

# ---- do the same if clustering.
if smartCluster == True:
  mergeclusterednode.set_dir_prefix("on_source/")
  mergeclusterednode.set_sn_flag("0 " + mergingCutsString)
  mergeclusterednode.set_sc_flag("1")
  mergeclusterednode.set_retry(retryNumberClustered)
  mergeclusterednode.set_name("xmergeclustered_on_source_" + mergeclusterednode.get_name())
  dag_clustered.add_node(mergeclusterednode)

# ---- Write out the submit files needed by condor.
dag.write_sub_files()
# ---- Write out the DAG itself.
dag.write_dag()
# ---- Delete used dag job
del dag
del job
del mergejob
# --- if we cluster
if smartCluster == True:
  dag_clustered.write_sub_files()
  dag_clustered.write_dag()
  del dag_clustered
  del mergeclusteredjob
# --- close on source distribute file
if distributeOnSource == 1 :
    nodeList.close()

# -------------------------------------------------------------------------
#      Write ul-source dag.
# -------------------------------------------------------------------------

# ---- Create a dag to which we can add jobs.
dag = pipeline.CondorDAG(log_file_on_source + uuidtag)

# ---- Set the name of the file that will contain the DAG.
dag.set_dag_file( 'grb_ul_source' )

# ---- Make instance of XsearchJob.
job = xcondor.XsearchJob(cp)

if outputType == 'seedless':
    if doGPUUL == "true":
        job = xcondor.XsearchGPUJob(cp)

# ---- Make instance of XmergeJob.
mergejob = xcondor.XmergeJob(cp)
# ---- Put a single merge job node in mergejob.  This job will combine
#      into a single file the output of all of the off-source analysis jobs.
mergenode = xcondor.XmergeNode(mergejob)
if xtmvaFlag:
    xtmvanode.add_parent(mergenode)

# ---- Make analysis jobs for all segments (currently 1) in the on-source
#      segment list.
segmentList = pipeline.ScienceData()
segmentList.read( 'input/segment_on_source.txt' , blockTime )

# ---- Read the number of chunks from the event list
event_on_file = open('input/event_on_source.txt')
event_on_list = event_on_file.readlines()
nOnEvents = len(event_on_list)
event_on_file.close()

# ---- Add one node to the job for each segment to be analysed.
for i in range(len(segmentList)):
    node = xcondor.XsearchNode(job)
    # ---- Parameters file:
    matlab_param_file = cwdstr + "/input/parameters_ul_source_" + str(i) + ".txt"
    node.set_param_file(matlab_param_file)
    node.set_x_jobnum(i)
    # ---- Ul-source results files are always written to the local
    #      output/ul_source directory - there are very few such jobs.
    node.set_output_dir( os.path.join( cwdstr + "/output/ul_source" ) )
    node.set_x_injnum('0')
    mergenode.add_parent(node)
    node.set_retry(retryNumber)
    # ---- Prepend human readable description to node name.
    node.set_name("xdetection_ul_source_seg" + str(i) + "_" + node.get_name())
    dag.add_node(node)

# ---- Supply remaining XmergeJob node parameters, add job to the dag.
mergenode.set_dir_prefix("ul_source/")
mergenode.set_sn_flag("0 " + mergingCutsString)
mergenode.set_retry(retryNumber)
# ---- Prepend human readable description to node name.
mergenode.set_name("xmerge_ul_source_" + mergenode.get_name())
dag.add_node(mergenode)


# ---- Write out the submit files needed by condor.
dag.write_sub_files()
# ---- Write out the DAG itself.
dag.write_dag()
# ---- Delete used dag job
del dag
del job
del mergejob

# -------------------------------------------------------------------------
#      Write off-source dag.
# -------------------------------------------------------------------------

# ---- Create a dag to which we can add jobs.
dag = pipeline.CondorDAG(log_file_off_source + uuidtag)

# ---- Set the name of the file that will contain the DAG.
dag.set_dag_file( 'grb_off_source' )

# ---- Make instance of XsearchJob.
job = xcondor.XsearchJob(cp)

if outputType == 'seedless':
    if doGPUOffSource == "true":
        job = xcondor.XsearchGPUJob(cp)

# ---- Make instance of XmergeJob.
mergejob = xcondor.XmergeJob(cp)
# ---- Put a single merge job node in mergejob.  This job will combine
#      into a single file the output of all of the off-source analysis jobs.
mergenode = xcondor.XmergeNode(mergejob)

if xtmvaFlag:
    xtmvanode.add_parent(mergenode)

# ---- Load off-source segments.
#segmentList = pipeline.ScienceData()
#segmentList.read( 'input/segment_off_source.txt', blockTime )

# ---- KLUDGE FIX: now we have option of limiting number of
#      background jobs using ini file we should ensure that
#      we only run the correct number of jobs.
event_off_file = open('input/event_off_source.txt')
event_off_list = event_off_file.readlines()
nOffEvents = len(event_off_list)
event_off_file.close()
# ---- END OF KLUDGE FXI

# ---- Check how many segments are to be bundled into each job, and create
#      job nodes accordingly.
maxInjNum = int(cp.get('output','maxInjNum'))
if cp.has_option('output','maxOffNum'):
    maxOffNum = int(cp.get('output','maxOffNum'))
else:
    maxOffNum = maxInjNum

# ---- Rescale by the number of sky position so that all jobs have the
#      same length regardless of the number of sky position
maxOffNum = math.ceil(float(maxOffNum)/math.sqrt(float(numSky)))
maxInjNum = math.ceil(float(maxInjNum)/math.sqrt(float(numSky)))

if maxOffNum == 0 :
    # ---- Each segment is to be analysed as a separate job.  There may be
    #      a lot of them, so we will check below if the output files are to
    #      be distributed over the cluster nodes.
    for i in range(nOffEvents):
        node = xcondor.XsearchNode(job)
        matlab_param_file = cwdstr + "/input/parameters_off_source.txt"
        node.set_param_file(matlab_param_file)
        node.set_x_jobnum(i)
        # ---- Output file to local output/off_source directory.
        node.set_output_dir( os.path.join( cwdstr + "/output/off_source" ) )
        node.set_x_injnum('0')
        mergenode.add_parent(node)
        node.set_retry(retryNumber)
        # ---- Prepend human readable description to node name.
        node.set_name("xdetection_off_source_seg" + str(i) + "_" + node.get_name())
        dag.add_node(node)
else:
    # ---- This option bundles together segments so that maxOffNum segments
    #      are analysed by each condor job node.
    #      In this case all output files come back to the local output/off_source
    #      directory.
    nSets = int(math.ceil(float(nOffEvents)/float(maxOffNum)))
    for i in range(nSets) :
        node = xcondor.XsearchNode(job)
        matlab_param_file = cwdstr + "/input/parameters_off_source_" + str(int(math.floor((jobsPerWindow*i)/nSets))) + ".txt"
        node.set_param_file(matlab_param_file)
        node.set_x_jobnum("%d-"%(i*maxOffNum) + "%d"%(min((i+1)*maxOffNum-1,nOffEvents-1)))
        node.set_output_dir( os.path.join( cwdstr + "/output/off_source" ) )
        node.set_x_injnum('0')
        mergenode.add_parent(node)
        node.set_retry(retryNumber)
        # ---- Prepend human readable description to node name.
        node.set_name("xdetection_off_source_seg" + str(i) + "_" + node.get_name())
        dag.add_node(node)

# ---- Supply remaining XmergeJob node parameters, add job to the dag.
mergenode.set_dir_prefix("off_source/")
mergenode.set_sn_flag("0 " + mergingCutsString)
mergenode.set_retry(retryNumber)
# ---- Prepend human readable description to node name.
mergenode.set_name("xmerge_off_source_" + mergenode.get_name())
dag.add_node(mergenode)
# ---- do the same if clustering.
if smartCluster == True:
  logger.info("Writing off-source clustered dag ... ")
  dag_clustered = pipeline.CondorDAG(log_file_off_source_clustered + uuidtag)
  dag_clustered.set_dag_file( 'grb_off_source_clustered' )
  mergeclusteredjob = xcondor.XmergeClusteredJob(cp,len(detector))
  mergeclusterednode = xcondor.XmergeClusteredNode(mergeclusteredjob)
  mergeclusterednode.set_dir_prefix("off_source/")
  mergeclusterednode.set_sn_flag("0 " + mergingCutsString)
  mergeclusterednode.set_sc_flag("1")
  mergeclusterednode.set_retry(retryNumberClustered)
  mergeclusterednode.set_name("xmergeclustered_off_source_" + mergeclusterednode.get_name())
  dag_clustered.add_node(mergeclusterednode)

# ---- Write out the submit files needed by condor.
dag.write_sub_files()
# ---- Write out the DAG itself.
dag.write_dag()
# ---- Delete used dag and jobs
del dag
del job
del mergejob
# --- Close file recording distribution of off-source results.
if smartCluster == True:
  dag_clustered.write_sub_files()
  dag_clustered.write_dag()
  del dag_clustered
  del mergeclusteredjob

# -------------------------------------------------------------------------
#      Write on-the-fly simulations dags - one for each waveform set.
# -------------------------------------------------------------------------

# ---- All injection scales for a given waveform set will be handled by a
#      single dag.
if cp.has_section('waveforms') :

    # ---- Read [injection] parameters.
    waveform_set = cp.options('waveforms')

    # ---- Read how distribute results on node and write to file
    distributeSimulation = int(cp.get('output','distributeSimulation'))

    if distributeSimulation == 1 :
        nodePath = cp.get('output','nodePath')
        onNodeSimulationPath = cp.get('output','onNodeSimulationPath')
        nNodes = int(cp.get('output','nNodes'))
        jobNodeFileSimulationPrefix = cp.get('output','jobNodeFileSimulationPrefix')
        nodeOffset = int(cp.get('output','numberOfFirstNode'));
        jobNumber = nodeOffset-1

    # ---- Write one dag for each waveform set.
    for set in waveform_set :
        # ---- The user can specify different injection scales for each
        #      waveform set by including a section with that name in the ini
        #      file.  Look for one.  If no special section for this waveform,
        #      then use the default injection scales from [injection].
        if cp.has_section(set) & cp.has_option(set,'injectionScales') :
            injectionScalesList = cp.get(set,'injectionScales')
            injectionScales = injectionScalesList.split(',')
        else:
            injectionScalesList = cp.get('injection','injectionScales')
            injectionScales = injectionScalesList.split(',')

        # ---- Create a dag to which we can add jobs.
        dag = pipeline.CondorDAG( log_file_simulations + "_" + set + uuidtag )

        # ---- Set the name of the file that will contain the DAG.
        dag.set_dag_file( "grb_simulations_" + set )

        # ---- Make instance of XsearchJob.
        job = xcondor.XsearchJob(cp)

        if outputType == 'seedless':
            if doGPUSimulation == "true":
                job = xcondor.XsearchGPUJob(cp)

        mergejob = xcondor.XmergeJob(cp)

        if smartCluster == True:
          logger.info('Writing dag for on-the-fly clustered injection set: ' + set)
          dag_clustered = pipeline.CondorDAG( log_file_simulations_clustered + "_" + set + uuidtag )
          dag_clustered.set_dag_file( "grb_simulations_clustered_" + set )
          mergeclusteredjob = xcondor.XmergeClusteredJob(cp,len(detector))

        # ---- Make analysis jobs for all segments in the on-source segment list.
        #      Read segment list from file.
        segmentList = pipeline.ScienceData()
        segmentList.read( 'input/segment_on_source.txt' , blockTime )

        # ---- Read injection file to determine number of injections
        #      for this set.
        #      Use system call to wc to figure out how many lines are
        #      in the injection file.
        os.system('wc input/injection_' + set + \
            '.txt | awk \'{print $1}\' > input/' + set + '.txt' )
        f = open('input/' + set + '.txt')
        numberOfInjections = int(f.readline())
        f.close()
        os.system('cat input/injection_' + set + \
                  '.txt | awk \'{printf \"%.9f\\n\",$1+$2*1e-9}\'' + \
                  ' > input/gps' + set + '.txt' )
        f = open('input/gps' + set + '.txt')
        injection_list_time = f.readlines()
        f.close()

        if longInjections == True:
          # ---- this method appends to an existing trigger file whatever bits of a long
          # waveform are in adjacent segments
          # The functionality is as follows: injection X is done first in segment 0 with parameters_0
          # then if straddling next segment is done in segment 1 with parameters_1 but the results are
          # appended to the existing trigger file
          mergenode = []
          if smartCluster == True:
            mergeclusterednode = []
          for injectionScale in injectionScales :
              mergenode.append(xcondor.XmergeNode(mergejob))
              if smartCluster == True:
                mergeclusterednode.append(xcondor.XmergeClusteredNode(mergeclusteredjob))
          if not(reUseInj):
            # ---- Set buffer for handling long injections.
            longInjSegBuffer = 0
            if set.startswith('adi-a') :
              longInjSegBuffer = 20
            if set.startswith('adi-b') :
              longInjSegBuffer = 6
            if set.startswith('adi-c') :
              longInjSegBuffer = 132
            if set.startswith('adi-d') :
              longInjSegBuffer = 74
            if set.startswith('adi-e') :
              longInjSegBuffer = 43
            if set.startswith('mva') :
              longInjSegBuffer = 132
            if set.startswith('mediummixed') :
              longInjSegBuffer = 6
            if set.startswith('longmixed') :
              longInjSegBuffer = 132
            if set.startswith('ebbh-a') :
              longInjSegBuffer = 7
            if set.startswith('ebbh-d') :
              longInjSegBuffer = 4
            if set.startswith('ebbh-e') :
              longInjSegBuffer = 4

            # ---- simplified method to inject, a la allsky.py
            for iInj in range(numberOfInjections) :
              thisInjSeg = []
              for i in range(len(segmentList)):
                if abs(float(event_on_list[i].split(' ')[0]) - float(injection_list_time[iInj]))<=blockTime/2-transientTime+longInjSegBuffer :
                  thisInjSeg.append(i)

              nodeList = []
              for i in range(len(thisInjSeg)):
                if thisInjSeg :
                  thisInjSeg.sort()
                  node = xcondor.XsearchNode(job)
                  matlab_param_file = cwdstr + "/input/parameters_simulation_" + set + "_0_" + str(thisInjSeg[i]) + ".txt"
                  node.set_param_file(matlab_param_file)
                  # --- method of grouping together segments for one condor job
                  #node.set_x_jobnum("%d-"%(thisInjSeg[0]) + "%d"%(thisInjSeg[-1]))
                  # --- we use multiple jobs but output written to a common trigger mat file
                  node.set_x_jobnum(thisInjSeg[i])
                  node.set_output_dir( os.path.join( cwdstr + '/output/simulations_' + set + '_' ) )
                  node.set_x_injnum(str(iInj+1))
                  for scale_counter in range(len(injectionScales)) :
                    mergenode[scale_counter].add_parent(node)
                  node.set_retry(retryNumber)
                  # ---- Prepend human readable description to node name.
                  node.set_name("xdetection_simulation_" + set + "_inj" + str(iInj+1) + "_" + node.get_name())
                  if len(thisInjSeg) == 1:
                    # ---- Add job node to dag
                    dag.add_node(node)
                  else:
                    # ---- Add job node to a list for parent-child relation
                    nodeList.append(node)

              # --- if we have long injections make sure chunks are always added after the previous chunk
              # --- in order of their segments, so xappendinjection can be applied correctly
              if len(thisInjSeg) > 1:
                for j in range(len(thisInjSeg) - 1):
                  parentNode = nodeList[j]
                  childNode = nodeList[j+1]
                  childNode.add_parent(parentNode)
                  dag.add_node(parentNode)
                dag.add_node(nodeList[-1])


          # ---- Loop over injection scales.
          for scale_counter in range(len(injectionScales)) :
              # ---- Point to already produced injection results if provided.
              if reUseInj:
                  mergenode[scale_counter].set_dir_prefix(mergingCutsPath + '/../output/simulations_' \
                      + set + '_' + str(scale_counter) + '/')
              else :
                 mergenode[scale_counter].set_dir_prefix('simulations_' + set + '_' + str(scale_counter) + '/')
              mergenode[scale_counter].set_sn_flag("1 " + mergingCutsString)
              mergenode[scale_counter].set_retry(retryNumber)
              # ---- Prepend human readable description to node name.
              mergenode[scale_counter].set_name("xmerge_simulation_" + set + "_injScale" \
                  + str(scale_counter) + "_" + mergenode[scale_counter].get_name())
              dag.add_node(mergenode[scale_counter])
              if smartCluster == True:
                # ---- Point to already produced injection results if provided.
                if reUseInj:
                    mergeclusterednode[scale_counter].set_dir_prefix(mergingCutsPath + '/../output/simulations_' \
                        + set + '_' + str(scale_counter) + '/')
                else :
                    mergeclusterednode[scale_counter].set_dir_prefix('simulations_' + set + '_' + str(scale_counter) + '/')
                mergeclusterednode[scale_counter].set_sn_flag("1 " + mergingCutsString)
                mergeclusterednode[scale_counter].set_sc_flag("1")
                mergeclusterednode[scale_counter].set_retry(retryNumberClustered)
                # ---- Prepend human readable description to node name.
                mergeclusterednode[scale_counter].set_name("xmergeclustered_simulation_" + set + "_injScale" \
                    + str(scale_counter) + "_" + mergeclusterednode[scale_counter].get_name())
                dag_clustered.add_node(mergeclusterednode[scale_counter])
        else:
          # ---- Loop over injection scales.
          scale_counter = 0
          for injectionScale in injectionScales :
            mergenode = xcondor.XmergeNode(mergejob)
            if xtmvaFlag:
              xtmvanode.add_parent(mergenode)
            if smartCluster == True:
              mergeclusterednode = xcondor.XmergeClusteredNode(mergeclusteredjob)
            # Skip injection trigger production if path to already available triggers provided
            if not(reUseInj):
              # ---- Loop over segments
              for i in range(len(segmentList)):
                # ---- For debug only
                #print('We do not have injections that are done at the block border, we use standard GRB injection placing')
                if  0 == maxInjNum:
                    # ---- Create job node.
                    node = xcondor.XsearchNode(job)
                    # ---- Assign first argument: parameters file
                    matlab_param_file = cwdstr + "/input/parameters_simulation_" \
                        + set + "_" + str(scale_counter) + '_' + str(i) + ".txt"
                    node.set_param_file(matlab_param_file)
                    # ---- Assign second argument: job (segment) number
                    #node.set_x_jobnum("%d-"%(thisInjSeg[0]) + "%d"%(thisInjSeg[-1]))
                    node.set_x_jobnum(i)
                    # ---- Assign third argument: output directory
                    # ---- Check to see if distributing results files over nodes.
                    if 1 == distributeSimulation & 0 == maxInjNum:
                        # ---- Yes: determine output directories and make sure they exist.
                        #      Record directories in a file.
                        jobNumber = (jobNumber+1-nodeOffset)%nNodes + nodeOffset
                        nodeList = open( jobNodeFileSimulationPrefix + '_' + set \
                            + "_seg%d"%(i) + "_injScale" + injectionScale + '.txt'  ,'w')
                        nodeList.write("Number_of_detectors %d\n"%(len(detector)))
                        for j in range(0,len(detector)) :
                            nodeList.write("%s\t"%(detector[j]))
                        nodeList.write("\n")
                        nodeList.write('event_simulation_file ')
                        nodeList.write(cwdstr + "/input/event_on_source.txt \n")
                        nodeList.write('Injection_file ')
                        nodeList.write(cwdstr + "/input/injection_" + set + ".txt \n")
                        nodeList.write('Number_of_jobs ')
                        nodeList.write("%d \n"%(numberOfInjections))
                        #nJobsPerNode = int(numberOfInjections/nNodes) + 1
                        nJobsPerNode = numberOfInjections
                        nodeList.write('Number_of_jobs_per_node ')
                        nodeList.write("%d \n"%(nJobsPerNode))
                        # write path for each result file, on each node write results from
                        # nJobsPerNode jobs
                        #jobNumber = int((injectionNumber-1)/nJobsPerNode) + nodeOffset
                        while ~(os.path.isdir (nodePath + "%d/"%(jobNumber))) &1:
                            logger.info("Waiting for automount to unmount ...\n")
                            time.sleep(10)
                        # Write name of node before changing nodes
                        #if 0 == (injectionNumber-1) % nJobsPerNode:
                        logger.info("Node number %d"%(jobNumber))
                        nodeList.write(nodePath + "%d/ \n"%(jobNumber))
                        # Create directory for results files
                        fullPath = nodePath + "%d/"%(jobNumber) + onNodeSimulationPath \
                            + "simulations_" + set + '_' + str(scale_counter)
                        if ~(os.path.isdir (fullPath))& 1 :
                            os.makedirs(fullPath)
                        else:
                            logger.warning("**WARNING** path: " + fullPath \
                            + " already exists, previous results may be overwritten\n")
                        node.set_output_dir(os.path.join( fullPath))
                        for injectionNumber in range(1,numberOfInjections+1) :
                            nodeList.write(fullPath + "/results_%d"%(i) \
                                +  "_%d.mat \n"%(injectionNumber) )
                        nodeList.close()
                    else:
                        # ---- No: Set output directory to local.
                        node.set_output_dir( os.path.join( cwdstr + \
                            '/output/simulations_' + set + '_' + str(scale_counter) ) )
                        # ---- Assign fourth argument: injection number
                        #      KLUDGE: This will screw up injections if more than
                        #      one segment; injection number iterates only over
                        #      injections that fall within analysis interval.
                    node.set_x_injnum("1-%d"%(numberOfInjections))
                    mergenode.add_parent(node)
                    node.set_retry(retryNumber)
                    # ---- Prepend human readable description to node name.
                    node.set_name("xdetection_simulation_" + set + "_seg" \
                        + str(i) + "_injScale" + str(scale_counter) + "_" + node.get_name())
                    dag.add_node(node)
                else:
                    for iWindow in range(len(injCenterTime)):
                        thisSegmentInjStart = 1e10
                        thisSegmentInjNumber = 0
                        for iInj in range(numberOfInjections) :
                            if abs(float(event_inj_list[i+iWindow*jobsPerWindow][0])-float(injection_list_time[iInj]))<=blockTime/2-transientTime :
                                thisSegmentInjNumber += 1
                                thisSegmentInjStart = min(thisSegmentInjStart,iInj)
                        for iInjRange in range(int(math.ceil(float(thisSegmentInjNumber)/float(maxInjNum)))) :
                            node = xcondor.XsearchNode(job)
                            matlab_param_file = cwdstr + "/input/parameters_simulation_" \
                                + set + "_" + str(scale_counter) + "_" + str(i) + ".txt"
                            node.set_param_file(matlab_param_file)
                            node.set_x_jobnum(i+iWindow*jobsPerWindow)
                            node.set_output_dir( os.path.join( cwdstr \
                                + '/output/simulations_' + set + '_' + str(scale_counter) ) )
                            node.set_x_injnum("%d"%(thisSegmentInjStart+iInjRange*maxInjNum+1) + "-" \
                                + "%d"%(thisSegmentInjStart+min((iInjRange+1)*maxInjNum,thisSegmentInjNumber)))
                            mergenode.add_parent(node)
                            node.set_retry(retryNumber)
                            # ---- Prepend human readable description to node name.
                            node.set_name("xdetection_simulation_" + set +  "_seg" + \
                                str(i+iWindow*jobsPerWindow) + "_injScale" + str(scale_counter) \
                                + "_injRange" + str(iInjRange) + "_" + node.get_name())
                            dag.add_node(node)
                            # ---- Add job node to the dag.
            # ---- Continue on to the next injection scale.
            # point to already produced injection results if provided
            if reUseInj:
                mergenode.set_dir_prefix(mergingCutsPath + '/../output/simulations_' + set + '_' + str(scale_counter) + '/')
            else:
                mergenode.set_dir_prefix('simulations_' + set + '_' + str(scale_counter) + '/')
            mergenode.set_sn_flag("0 " + mergingCutsString)
            mergenode.set_retry(retryNumber)
            # ---- Prepend human readable description to node name.
            mergenode.set_name("xmerge_simulation_" + set + "_injScale" \
                            + str(scale_counter) + "_" + mergenode.get_name())
            dag.add_node(mergenode)
            if smartCluster == True:
              # ---- Point to already produced injection results if provided.
              if reUseInj:
                  mergeclusterednode.set_dir_prefix(mergingCutsPath + '/../output/simulations_' \
                      + set + '_' + str(scale_counter) + '/')
              else :
                  mergeclusterednode.set_dir_prefix('simulations_' + set + '_' + str(scale_counter) + '/')
              mergeclusterednode.set_sn_flag("1 " + mergingCutsString)
              mergeclusterednode.set_sc_flag("1")
              mergeclusterednode.set_retry(retryNumberClustered)
              # ---- Prepend human readable description to node name.
              mergeclusterednode.set_name("xmergeclustered_simulation_" + set + "_injScale" \
                  + str(scale_counter) + "_" + mergeclusterednode.get_name())
              dag_clustered.add_node(mergeclusterednode)
            scale_counter = scale_counter + 1

        # ---- Write out the submit files needed by condor.
        dag.write_sub_files()
        # ---- Write out the DAG itself.
        dag.write_dag()

        # ---- Delete used dag job
        del dag
        del job
        del mergejob
        if smartCluster == True:
          dag_clustered.write_sub_files()
          dag_clustered.write_dag()
          del dag_clustered
          del mergeclusteredjob

# -------------------------------------------------------------------------
#      Write MDC simulation dags - one for each MDC set.
# -------------------------------------------------------------------------

# ---- All injection scales for a given MDC set will be handled by a
#      single dag.

# ---- Check for MDC sets.
if cp.has_option('mdc','mdc_sets') :

    # ---- Status message.
    logger.info("Writing MDC job dag files ... ")

    # ---- Get list of MDC sets to process.
    mdc_setsList = cp.get('mdc','mdc_sets')
    mdc_sets = mdc_setsList.split(',')

    # ---- Check how many MDC segments are to be bundled into each xdetection job.
    maxMDCSegNum = int(cp.get('output','maxMDCSegNum'))

    # ---- Read how the user wants the results distributed (over nodes, or not)
    #      and record this info to a file.
    distributeSimulation = int(cp.get('output','distributeSimulation'))
    if distributeSimulation == 1 :
        nodePath = cp.get('output','nodePath')
        onNodeSimulationPath = cp.get('output','onNodeSimulationPath')
        nNodes = int(cp.get('output','nNodes'))
        jobNodeFileSimulationPrefix = cp.get('output','jobNodeFileSimulationPrefix')
        nodeOffset = int(cp.get('output','numberOfFirstNode'));
        jobNumber = nodeOffset-1

    # ---- Write one dag for each waveform set.
    for set in mdc_sets :
        logger.info('Writing dag for MDC set: {0}'.format(set))
        if cp.has_section(set) & cp.has_option(set,'injectionScales') :
            injectionScalesList = cp.get(set,'injectionScales')
            injectionScales = injectionScalesList.split(',')
        else:
            injectionScalesList = cp.get('injection','injectionScales')
            injectionScales = injectionScalesList.split(',')

        # ---- Create a dag to which we can add jobs.
        dag = pipeline.CondorDAG( log_file_simulations + "_" + set + uuidtag)

        # ---- Set the name of the file that will contain the DAG.
        dag.set_dag_file( "grb_" + set )

        # ---- Make instance of XsearchJob.
        job = xcondor.XsearchJob(cp)

        # ---- Make instance of XmergeJob. This job will merge the results files
        #      produced by the several analysis nodes.
        mergejob = xcondor.XmergeJob(cp)

        #      Make analysis jobs for all segments in the MDC segment list.
        #      Read segment list from file.
        segFileName = 'input/segment_' + set +  '.txt'
        if not os.path.isfile(segFileName):
            logger.error("Error: non existant segment file: {0}".format(segFileName))

        segmentList = []
        segmentList = pipeline.ScienceData()
        segmentList.read( segFileName , blockTime )

        # ---- Read ini file to determine number of injections for this set.
        numberOfInjections = int(cp.get(set,'numberOfChannels'))

        # ---- Loop over injection scales.
        scale_counter = 0
        for injectionScale in injectionScales :
            # ---- Merge all results for a single waveform+injection scale.
            mergenode = xcondor.XmergeNode(mergejob)
            if xtmvaFlag:
                xtmvanode.add_parent(mergenode)

            # ---- Set up of jobs depends on how many injections are to be done
            #      by each job: a fixed number (maxInjNum,maxMDCSegNum), or all
            #      (maxInjNum==0,maxMDCSegNum==0).
            if maxInjNum == 0 and maxMDCSegNum == 0:
                # ---- Each analysis job will analyse any and all injections in
                #      its analysis segment.
                i = 0

                # ---- Create job node.
                node = xcondor.XsearchNode(job)
                # ---- Assign first argument: parameters file
                matlab_param_file = cwdstr + "/input/parameters_" + set \
                    + "_" + str(scale_counter) + ".txt"
                node.set_param_file(matlab_param_file)
                # ---- Assign second argument: job (segment) number
                node.set_x_jobnum(i)
                # ---- Assign third argument: output directory
                # ---- Check to see if distributing results files over nodes.
                if 1 == distributeSimulation & 0 == maxInjNum:
                    # ---- Yes: determine output directories and make sure they exist.
                    #      Record directories in a file.
                    jobNumber = (jobNumber+1-nodeOffset)%nNodes + nodeOffset
                    nodeList = open( jobNodeFileSimulationPrefix + '_' + set \
                        + "_seg%d"%(i) + "_injScale" + injectionScale + '.txt'  ,'w')
                    nodeList.write("Number_of_detectors %d\n"%(len(detector)))
                    for j in range(0,len(detector)) :
                        nodeList.write("%s\t"%(detector[j]))
                    nodeList.write("\n")
                    nodeList.write('event_simulation_file ')
                    nodeList.write(cwdstr + "/input/event_on_source.txt \n")
                    nodeList.write('Injection_file ')
                    nodeList.write(cwdstr + "/input/injection_" + set + ".txt \n")
                    nodeList.write('Number_of_jobs ')
                    nodeList.write("%d \n"%(numberOfInjections))
                    nJobsPerNode = numberOfInjections
                    nodeList.write('Number_of_jobs_per_node ')
                    nodeList.write("%d \n"%(nJobsPerNode))
                    # write path for each result file, on each node write results from
                    # nJobsPerNode jobs
                    #jobNumber = int((injectionNumber-1)/nJobsPerNode) + nodeOffset
                    while ~(os.path.isdir (nodePath + "%d/"%(jobNumber))) &1:
                        logger.info("Waiting for automount to unmount ...\n")
                        time.sleep(10)
                    # Write name of node before changing nodes
                    #if 0 == (injectionNumber-1) % nJobsPerNode:
                    logger.info("Node number %d"%(jobNumber))
                    nodeList.write(nodePath + "%d/ \n"%(jobNumber))
                    # Create directory for results files
                    fullPath = nodePath + "%d/"%(jobNumber) + onNodeSimulationPath \
                        + "_" + set + '_' + str(scale_counter)
                    if ~(os.path.isdir (fullPath))& 1 :
                        os.makedirs(fullPath)
                    else:
                        logger.warning("**WARNING** path: " + fullPath + \
                            " already exists, previous results may be overwritten\n")
                    node.set_output_dir(os.path.join( fullPath))
                    for injectionNumber in range(1,numberOfInjections+1) :
                        nodeList.write(fullPath + "/results_%d"%(i) +  \
                            "_%d.mat \n"%(injectionNumber) )
                    nodeList.close()
                else:
                    # ---- No: Set output directory to local.
                    node.set_output_dir( os.path.join( cwdstr + \
                        '/output/simulations_' + set + '_' + str(scale_counter) ) )
                # ---- Assign fourth argument: injection number
                #      KLUDGE: This will screw up injections if more than
                #      one segment; injection number iterates only over
                #      injections that fall within analysis interval.
                node.set_x_injnum("1-%d"%(numberOfInjections))
                mergenode.add_parent(node)
                node.set_retry(retryNumber)
                # ---- Prepend human readable description to node name.
                node.set_name("xdetection_simulation_" + set + "_injScale" \
                    + str(scale_counter) + "_" + node.get_name())
                dag.add_node(node)
            # ---- if maxInjNum and maxMDCSegNum not 0
            else:
                # ---- Analyse maxInjNum injections in each job.  In this case output
                #      files will go to the local directory output/simulations_*.
                segJobs = range(int(math.ceil(float(len(segmentList))/float(maxMDCSegNum))))
                injJobs = range(int(math.ceil(float(numberOfInjections)/float(maxInjNum))))
                for iSegRange in segJobs :
                    for iInjRange in injJobs :
                        node = xcondor.XsearchNode(job)
                        matlab_param_file = cwdstr + "/input/parameters_" + set \
                            + "_" + str(scale_counter) + ".txt"
                        node.set_param_file(matlab_param_file)

                        if len(segJobs)==1:
                            # ---- if we only have one segment
                            node.set_x_jobnum('0')
                        else:
                            node.set_x_jobnum("%d"%(iSegRange*maxMDCSegNum) + "-" + \
                                "%d"%(min((iSegRange+1)*maxMDCSegNum-1,len(segmentList)-1)))

                        node.set_output_dir( os.path.join( cwdstr + '/output/simulations_' \
                            + set + '_' + str(scale_counter) ) )

                        # ---- WARNING - do not set injNum = 0 when running MDCs or
                        #      we will not recreate the channel name properly in xdetection
                        node.set_x_injnum("%d"%(iInjRange*maxInjNum+1) + "-" \
                            + "%d"%(min((iInjRange+1)*maxInjNum,numberOfInjections)))

                        mergenode.add_parent(node)
                        node.set_retry(retryNumber)
                        # ---- Prepend human readable description to node name.
                        node.set_name("xdetection_simulation_" + set + "_seg" \
                            + str(iSegRange) + "_injScale" + str(scale_counter) \
                            + "_injRange" + str(iInjRange) + "_" + node.get_name())
                        dag.add_node(node)

            # ---- Add job node to the dag.
            # ---- Continue on to the next injection scale.
            mergenode.set_dir_prefix('simulations_' + set + '_' + str(scale_counter) + '/')
            mergenode.set_sn_flag("0 " + mergingCutsString)
            mergenode.set_retry(retryNumber)
            # ---- Prepend human readable description to node name.
            mergenode.set_name("xmerge_simulation_" + set + "_seg" + \
                "_injScale" + str(scale_counter) + "_" + mergenode.get_name())
            dag.add_node(mergenode)
            scale_counter = scale_counter + 1

        # ---- end loop over injections scales

        # ---- Write out the submit files needed by condor.
        dag.write_sub_files()
        # ---- Write out the DAG itself.
        dag.write_dag()

        # ---- Delete used dag job
        del dag
        del job
        del mergejob
    # ---- end loop overmdc sets

# -------------------------------------------------------------------------
#      Write xtmva dag.
# -------------------------------------------------------------------------

if xtmvaFlag:
    dag_xtmva.add_node(xtmvanode)
    dag_xtmva.write_sub_files()
    dag_xtmva.write_dag()
    del dag_xtmva
    del xtmvajob

# -------------------------------------------------------------------------
#      Write single dag containing all jobs.
# -------------------------------------------------------------------------

# ---- Use grep to combine all dag files into a single dag, with the
#      PARENT-CHILD relationships listed at the end.
logger.info("Combining all jobs into a single dag ...")
os.system('rm -f grb_alljobs.dag')
os.system('echo "DOT xpipeline_triggerGen.dot" > .dag_temp')
os.system('grep -h -v PARENT *.dag >> .dag_temp')
os.system('grep -h PARENT *.dag >> .dag_temp')
os.system('mv .dag_temp grb_alljobs.dag')


# -------------------------------------------------------------------------
#      Finished.
# -------------------------------------------------------------------------

# ---- Status message.
logger.info("... finished writing job submission files. ")
logger.info("\n")

logger.info("############################################")
logger.info("#           Completed.                     #")
logger.info("############################################")

# ---- Append to summary file.
sfile = open(summary_file,'a')
sfile.write('\t 1')
sfile.close()

# ---- Exit cleanly
sys.exit( 0 )


# -------------------------------------------------------------------------
#      Leftover code samples.
# -------------------------------------------------------------------------

# # ---- Make data find job node.
# df_job = pipeline.LSCDataFindJob( 'cache','logs', cp )

# # ---- Make an empty list to hold the datafind jobs.
# df_list = []

# # ---- Loop over detectors and prepare a datafind job for each.
# for ifo in ['H1', 'H2', 'L1']:
#   df = pipeline.LSCDataFindNode( df_job )
#   df.set_start( 700000000 )
#   df.set_end( 700000100 )
#   df.set_observatory( ifo[0] )
#   df.set_type('RDS_R_L1')
#   df.set_post_script('/why/oh/why/oh/why.sh')
#   df.add_post_script_arg(df.get_output())
#   dag.add_node(df)
#   df_list.append(df)

